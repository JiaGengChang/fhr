{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "# os.chdir('/home/jiageng/Documents/fhr/pipeline/')\n",
    "os.chdir('/home/jiageng/Documents/fhr/pygcn/pygcn')\n",
    "import snf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdNormalize(df):\n",
    "    std = df.std().fillna(1)\n",
    "    mean = df - df.mean()\n",
    "    df_norm = mean / std\n",
    "    return np.array(df_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowNormalize(mx):\n",
    "    \"\"\"Row-normalize matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = np.diag(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "def sparse_tensor_from_arr(arr):\n",
    "    \"\"\"Convert numpy array to torch sparse tensor\"\"\"\n",
    "    # Convert numpy array to scipy sparse matrix\n",
    "    sparse_sp = coo_matrix(arr)\n",
    "    \n",
    "    # Convert scipy sparse matrix to torch sparse tensor\n",
    "    sparse_tensor = torch.sparse_coo_tensor(\n",
    "        torch.LongTensor([sparse_sp.row, sparse_sp.col]),\n",
    "        torch.FloatTensor(sparse_sp.data),\n",
    "        torch.Size(sparse_sp.shape)\n",
    "    )\n",
    "    \n",
    "    return sparse_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "fhr_data = pd.read_csv('/home/jiageng/Documents/fhr/annotations/fhr-annotations.tsv',sep='\\t').set_index('PUBLIC_ID').query('risk != -1')\n",
    "fhr_data['risk'] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('/home/jiageng/Documents/fhr/matrices/broad_cn_matrix.tsv',sep='\\t').set_index('PUBLIC_ID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>SeqWGS_Cp_10p14</th>\n",
       "      <th>SeqWGS_Cp_10q23</th>\n",
       "      <th>SeqWGS_Cp_11p15</th>\n",
       "      <th>SeqWGS_Cp_11q23</th>\n",
       "      <th>SeqWGS_Cp_12p13</th>\n",
       "      <th>SeqWGS_Cp_12q21</th>\n",
       "      <th>SeqWGS_Cp_13q14</th>\n",
       "      <th>SeqWGS_Cp_13q34</th>\n",
       "      <th>SeqWGS_Cp_14q23</th>\n",
       "      <th>SeqWGS_Cp_15q15</th>\n",
       "      <th>...</th>\n",
       "      <th>SeqWGS_Cp_5p15</th>\n",
       "      <th>SeqWGS_Cp_5q31</th>\n",
       "      <th>SeqWGS_Cp_6p22</th>\n",
       "      <th>SeqWGS_Cp_6q25</th>\n",
       "      <th>SeqWGS_Cp_7p14</th>\n",
       "      <th>SeqWGS_Cp_7q22</th>\n",
       "      <th>SeqWGS_Cp_8p22</th>\n",
       "      <th>SeqWGS_Cp_8q24</th>\n",
       "      <th>SeqWGS_Cp_9p13</th>\n",
       "      <th>SeqWGS_Cp_9q33</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>924.000000</td>\n",
       "      <td>924.000000</td>\n",
       "      <td>924.000000</td>\n",
       "      <td>924.000000</td>\n",
       "      <td>924.000000</td>\n",
       "      <td>924.000000</td>\n",
       "      <td>924.000000</td>\n",
       "      <td>924.000000</td>\n",
       "      <td>924.000000</td>\n",
       "      <td>924.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>924.000000</td>\n",
       "      <td>924.000000</td>\n",
       "      <td>924.000000</td>\n",
       "      <td>924.000000</td>\n",
       "      <td>924.000000</td>\n",
       "      <td>924.000000</td>\n",
       "      <td>924.000000</td>\n",
       "      <td>924.000000</td>\n",
       "      <td>924.000000</td>\n",
       "      <td>924.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.015098</td>\n",
       "      <td>0.026733</td>\n",
       "      <td>0.227654</td>\n",
       "      <td>0.277808</td>\n",
       "      <td>-0.068431</td>\n",
       "      <td>-0.013188</td>\n",
       "      <td>-0.401000</td>\n",
       "      <td>-0.341593</td>\n",
       "      <td>-0.158784</td>\n",
       "      <td>0.369517</td>\n",
       "      <td>...</td>\n",
       "      <td>0.273979</td>\n",
       "      <td>0.261853</td>\n",
       "      <td>0.140313</td>\n",
       "      <td>-0.092735</td>\n",
       "      <td>0.199197</td>\n",
       "      <td>0.216338</td>\n",
       "      <td>-0.149380</td>\n",
       "      <td>0.044667</td>\n",
       "      <td>0.340076</td>\n",
       "      <td>0.364253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.176041</td>\n",
       "      <td>0.148618</td>\n",
       "      <td>0.316424</td>\n",
       "      <td>0.334422</td>\n",
       "      <td>0.275947</td>\n",
       "      <td>0.194094</td>\n",
       "      <td>0.444205</td>\n",
       "      <td>0.436672</td>\n",
       "      <td>0.376785</td>\n",
       "      <td>0.377280</td>\n",
       "      <td>...</td>\n",
       "      <td>0.306626</td>\n",
       "      <td>0.311341</td>\n",
       "      <td>0.254870</td>\n",
       "      <td>0.386022</td>\n",
       "      <td>0.279982</td>\n",
       "      <td>0.275104</td>\n",
       "      <td>0.347613</td>\n",
       "      <td>0.299093</td>\n",
       "      <td>0.347467</td>\n",
       "      <td>0.326237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.381000</td>\n",
       "      <td>-0.954000</td>\n",
       "      <td>-0.978000</td>\n",
       "      <td>-0.934000</td>\n",
       "      <td>-1.271000</td>\n",
       "      <td>-1.084000</td>\n",
       "      <td>-1.941000</td>\n",
       "      <td>-1.036000</td>\n",
       "      <td>-0.994000</td>\n",
       "      <td>-0.286000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.938000</td>\n",
       "      <td>-0.849000</td>\n",
       "      <td>-0.965000</td>\n",
       "      <td>-1.005000</td>\n",
       "      <td>-0.976000</td>\n",
       "      <td>-0.241000</td>\n",
       "      <td>-1.200000</td>\n",
       "      <td>-0.974000</td>\n",
       "      <td>-0.964000</td>\n",
       "      <td>-0.293000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>-0.001250</td>\n",
       "      <td>0.005000</td>\n",
       "      <td>-0.911000</td>\n",
       "      <td>-0.882250</td>\n",
       "      <td>-0.087250</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>-0.030250</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>-0.131250</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.017000</td>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.015000</td>\n",
       "      <td>0.013000</td>\n",
       "      <td>0.027000</td>\n",
       "      <td>0.038000</td>\n",
       "      <td>0.012000</td>\n",
       "      <td>0.014000</td>\n",
       "      <td>-0.154000</td>\n",
       "      <td>-0.017500</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>0.327000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.035000</td>\n",
       "      <td>0.032500</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>0.011000</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>0.026000</td>\n",
       "      <td>0.008000</td>\n",
       "      <td>0.015500</td>\n",
       "      <td>0.401500</td>\n",
       "      <td>0.461500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.021000</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>0.549000</td>\n",
       "      <td>0.573250</td>\n",
       "      <td>0.021000</td>\n",
       "      <td>0.022000</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>0.009000</td>\n",
       "      <td>0.019000</td>\n",
       "      <td>0.594000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.567250</td>\n",
       "      <td>0.567000</td>\n",
       "      <td>0.107500</td>\n",
       "      <td>0.023000</td>\n",
       "      <td>0.534000</td>\n",
       "      <td>0.545000</td>\n",
       "      <td>0.016000</td>\n",
       "      <td>0.026250</td>\n",
       "      <td>0.583000</td>\n",
       "      <td>0.584000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.007000</td>\n",
       "      <td>1.019000</td>\n",
       "      <td>1.424000</td>\n",
       "      <td>1.764000</td>\n",
       "      <td>1.022000</td>\n",
       "      <td>1.022000</td>\n",
       "      <td>0.761000</td>\n",
       "      <td>0.774000</td>\n",
       "      <td>1.025000</td>\n",
       "      <td>1.602000</td>\n",
       "      <td>...</td>\n",
       "      <td>1.143000</td>\n",
       "      <td>1.052000</td>\n",
       "      <td>1.126000</td>\n",
       "      <td>1.025000</td>\n",
       "      <td>1.530000</td>\n",
       "      <td>1.530000</td>\n",
       "      <td>1.009000</td>\n",
       "      <td>3.053000</td>\n",
       "      <td>1.729000</td>\n",
       "      <td>1.574000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       SeqWGS_Cp_10p14  SeqWGS_Cp_10q23  SeqWGS_Cp_11p15  SeqWGS_Cp_11q23  \\\n",
       "count       924.000000       924.000000       924.000000       924.000000   \n",
       "mean          0.015098         0.026733         0.227654         0.277808   \n",
       "std           0.176041         0.148618         0.316424         0.334422   \n",
       "min          -1.381000        -0.954000        -0.978000        -0.934000   \n",
       "25%           0.006000         0.006000         0.014000         0.016000   \n",
       "50%           0.015000         0.013000         0.027000         0.038000   \n",
       "75%           0.021000         0.021000         0.549000         0.573250   \n",
       "max           1.007000         1.019000         1.424000         1.764000   \n",
       "\n",
       "       SeqWGS_Cp_12p13  SeqWGS_Cp_12q21  SeqWGS_Cp_13q14  SeqWGS_Cp_13q34  \\\n",
       "count       924.000000       924.000000       924.000000       924.000000   \n",
       "mean         -0.068431        -0.013188        -0.401000        -0.341593   \n",
       "std           0.275947         0.194094         0.444205         0.436672   \n",
       "min          -1.271000        -1.084000        -1.941000        -1.036000   \n",
       "25%          -0.001250         0.005000        -0.911000        -0.882250   \n",
       "50%           0.012000         0.014000        -0.154000        -0.017500   \n",
       "75%           0.021000         0.022000         0.009000         0.009000   \n",
       "max           1.022000         1.022000         0.761000         0.774000   \n",
       "\n",
       "       SeqWGS_Cp_14q23  SeqWGS_Cp_15q15  ...  SeqWGS_Cp_5p15  SeqWGS_Cp_5q31  \\\n",
       "count       924.000000       924.000000  ...      924.000000      924.000000   \n",
       "mean         -0.158784         0.369517  ...        0.273979        0.261853   \n",
       "std           0.376785         0.377280  ...        0.306626        0.311341   \n",
       "min          -0.994000        -0.286000  ...       -0.938000       -0.849000   \n",
       "25%          -0.087250         0.021000  ...        0.012000        0.014000   \n",
       "50%           0.009000         0.327000  ...        0.035000        0.032500   \n",
       "75%           0.019000         0.594000  ...        0.567250        0.567000   \n",
       "max           1.025000         1.602000  ...        1.143000        1.052000   \n",
       "\n",
       "       SeqWGS_Cp_6p22  SeqWGS_Cp_6q25  SeqWGS_Cp_7p14  SeqWGS_Cp_7q22  \\\n",
       "count      924.000000      924.000000      924.000000      924.000000   \n",
       "mean         0.140313       -0.092735        0.199197        0.216338   \n",
       "std          0.254870        0.386022        0.279982        0.275104   \n",
       "min         -0.965000       -1.005000       -0.976000       -0.241000   \n",
       "25%          0.012000       -0.030250        0.013000        0.014000   \n",
       "50%          0.021000        0.011000        0.024000        0.026000   \n",
       "75%          0.107500        0.023000        0.534000        0.545000   \n",
       "max          1.126000        1.025000        1.530000        1.530000   \n",
       "\n",
       "       SeqWGS_Cp_8p22  SeqWGS_Cp_8q24  SeqWGS_Cp_9p13  SeqWGS_Cp_9q33  \n",
       "count      924.000000      924.000000      924.000000      924.000000  \n",
       "mean        -0.149380        0.044667        0.340076        0.364253  \n",
       "std          0.347613        0.299093        0.347467        0.326237  \n",
       "min         -1.200000       -0.974000       -0.964000       -0.293000  \n",
       "25%         -0.131250        0.008000        0.017000        0.020000  \n",
       "50%          0.008000        0.015500        0.401500        0.461500  \n",
       "75%          0.016000        0.026250        0.583000        0.584000  \n",
       "max          1.009000        3.053000        1.729000        1.574000  \n",
       "\n",
       "[8 rows x 42 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([924, 42])\n"
     ]
    }
   ],
   "source": [
    "feature_norm_mtd = '' \n",
    "\n",
    "if feature_norm_mtd == 'stdnorm':\n",
    "    features = torch.tensor(stdNormalize(data),dtype=torch.float32)\n",
    "elif feature_norm_mtd == 'rownorm':\n",
    "    features = torch.tensor(rowNormalize(data.values),dtype=torch.float32)\n",
    "else:\n",
    "    features = torch.tensor(data.values,dtype=torch.float32)\n",
    "    \n",
    "print(features.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "924\n"
     ]
    }
   ],
   "source": [
    "risk = fhr_data.reindex(pd.Index(data.index)).loc[data.index]['risk'].fillna(-1).astype(int)\n",
    "labels = torch.tensor(risk.values,dtype=torch.long)\n",
    "print(len(labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "risk\n",
       " 1    513\n",
       " 2    259\n",
       " 3     92\n",
       "-1     60\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing values are set to -1\n",
    "risk.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assign training and validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un-labeled 60\n",
      "Labeled train 576\n",
      "Labeled val 288\n"
     ]
    }
   ],
   "source": [
    "idx_labeled = np.where(risk != -1)[0]\n",
    "idx_unlabeled = np.where(risk == -1)[0]\n",
    "idx_val = torch.tensor(idx_labeled[1::3]) # k-fold split\n",
    "idx_test = idx_val\n",
    "idx_train = torch.tensor([idx for idx in idx_labeled if idx not in idx_val])\n",
    "print('Un-labeled',len(idx_unlabeled))\n",
    "print('Labeled train',len(idx_train))\n",
    "print('Labeled val',len(idx_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiageng/Documents/fhr/.venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/jiageng/Documents/fhr/.venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(924, 924)\n"
     ]
    }
   ],
   "source": [
    "aff = snf.make_affinity(stdNormalize(data), metric='euclidean', K=20, mu=0.5)\n",
    "print(aff.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.97 0.013048945651615969\n"
     ]
    }
   ],
   "source": [
    "pctile = 0.97\n",
    "threshold = np.quantile(aff,pctile)\n",
    "print(pctile, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "aff_clip = aff.copy()\n",
    "aff_clip[aff_clip < threshold] = 0\n",
    "adj = sparse_tensor_from_arr(rowNormalize(aff_clip))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.4009 acc_train: 0.0000 loss_val: 1.3173 acc_val: 0.4792 time: 0.4861s\n",
      "Epoch: 0002 loss_train: 1.3462 acc_train: 0.4132 loss_val: 1.2691 acc_val: 0.6042 time: 0.7230s\n",
      "Epoch: 0003 loss_train: 1.3069 acc_train: 0.5260 loss_val: 1.2258 acc_val: 0.6250 time: 0.4668s\n",
      "Epoch: 0004 loss_train: 1.2666 acc_train: 0.5469 loss_val: 1.1861 acc_val: 0.6424 time: 0.4957s\n",
      "Epoch: 0005 loss_train: 1.2358 acc_train: 0.5660 loss_val: 1.1497 acc_val: 0.6424 time: 0.4776s\n",
      "Epoch: 0006 loss_train: 1.2026 acc_train: 0.5694 loss_val: 1.1144 acc_val: 0.6424 time: 0.4641s\n",
      "Epoch: 0007 loss_train: 1.1771 acc_train: 0.5694 loss_val: 1.0807 acc_val: 0.6424 time: 0.5088s\n",
      "Epoch: 0008 loss_train: 1.1460 acc_train: 0.5694 loss_val: 1.0490 acc_val: 0.6424 time: 0.4883s\n",
      "Epoch: 0009 loss_train: 1.1227 acc_train: 0.5694 loss_val: 1.0202 acc_val: 0.6424 time: 0.4775s\n",
      "Epoch: 0010 loss_train: 1.1017 acc_train: 0.5694 loss_val: 0.9947 acc_val: 0.6424 time: 0.5486s\n",
      "Epoch: 0011 loss_train: 1.0761 acc_train: 0.5694 loss_val: 0.9729 acc_val: 0.6424 time: 0.4827s\n",
      "Epoch: 0012 loss_train: 1.0632 acc_train: 0.5694 loss_val: 0.9548 acc_val: 0.6424 time: 0.4853s\n",
      "Epoch: 0013 loss_train: 1.0428 acc_train: 0.5694 loss_val: 0.9400 acc_val: 0.6424 time: 0.5478s\n",
      "Epoch: 0014 loss_train: 1.0328 acc_train: 0.5694 loss_val: 0.9280 acc_val: 0.6424 time: 0.4655s\n",
      "Epoch: 0015 loss_train: 1.0196 acc_train: 0.5694 loss_val: 0.9184 acc_val: 0.6424 time: 0.7475s\n",
      "Epoch: 0016 loss_train: 1.0161 acc_train: 0.5694 loss_val: 0.9107 acc_val: 0.6424 time: 0.5934s\n",
      "Epoch: 0017 loss_train: 1.0062 acc_train: 0.5694 loss_val: 0.9048 acc_val: 0.6424 time: 0.5398s\n",
      "Epoch: 0018 loss_train: 0.9954 acc_train: 0.5694 loss_val: 0.9004 acc_val: 0.6424 time: 0.5155s\n",
      "Epoch: 0019 loss_train: 0.9885 acc_train: 0.5694 loss_val: 0.8976 acc_val: 0.6424 time: 0.4894s\n",
      "Epoch: 0020 loss_train: 0.9808 acc_train: 0.5694 loss_val: 0.8961 acc_val: 0.6424 time: 0.5121s\n",
      "Epoch: 0021 loss_train: 0.9690 acc_train: 0.5694 loss_val: 0.8958 acc_val: 0.6424 time: 0.4796s\n",
      "Epoch: 0022 loss_train: 0.9730 acc_train: 0.5694 loss_val: 0.8955 acc_val: 0.6424 time: 0.7566s\n",
      "Epoch: 0023 loss_train: 0.9635 acc_train: 0.5694 loss_val: 0.8939 acc_val: 0.6424 time: 0.5357s\n",
      "Epoch: 0024 loss_train: 0.9604 acc_train: 0.5712 loss_val: 0.8900 acc_val: 0.6424 time: 0.4949s\n",
      "Epoch: 0025 loss_train: 0.9542 acc_train: 0.5694 loss_val: 0.8831 acc_val: 0.6424 time: 0.4820s\n",
      "Epoch: 0026 loss_train: 0.9372 acc_train: 0.5694 loss_val: 0.8749 acc_val: 0.6424 time: 0.4962s\n",
      "Epoch: 0027 loss_train: 0.9474 acc_train: 0.5694 loss_val: 0.8665 acc_val: 0.6424 time: 0.4870s\n",
      "Epoch: 0028 loss_train: 0.9425 acc_train: 0.5694 loss_val: 0.8606 acc_val: 0.6424 time: 0.4814s\n",
      "Epoch: 0029 loss_train: 0.9385 acc_train: 0.5694 loss_val: 0.8570 acc_val: 0.6424 time: 0.4763s\n",
      "Epoch: 0030 loss_train: 0.9270 acc_train: 0.5694 loss_val: 0.8548 acc_val: 0.6424 time: 0.4847s\n",
      "Epoch: 0031 loss_train: 0.9343 acc_train: 0.5694 loss_val: 0.8527 acc_val: 0.6424 time: 0.5400s\n",
      "Epoch: 0032 loss_train: 0.9313 acc_train: 0.5694 loss_val: 0.8520 acc_val: 0.6424 time: 0.5457s\n",
      "Epoch: 0033 loss_train: 0.9170 acc_train: 0.5694 loss_val: 0.8514 acc_val: 0.6424 time: 0.5180s\n",
      "Epoch: 0034 loss_train: 0.9180 acc_train: 0.5764 loss_val: 0.8492 acc_val: 0.6424 time: 0.5281s\n",
      "Epoch: 0035 loss_train: 0.9206 acc_train: 0.5677 loss_val: 0.8454 acc_val: 0.6424 time: 0.5565s\n",
      "Epoch: 0036 loss_train: 0.9150 acc_train: 0.5660 loss_val: 0.8424 acc_val: 0.6424 time: 0.5581s\n",
      "Epoch: 0037 loss_train: 0.9025 acc_train: 0.5764 loss_val: 0.8387 acc_val: 0.6354 time: 0.5975s\n",
      "Epoch: 0038 loss_train: 0.9017 acc_train: 0.5747 loss_val: 0.8336 acc_val: 0.6389 time: 0.6672s\n",
      "Epoch: 0039 loss_train: 0.8979 acc_train: 0.5868 loss_val: 0.8285 acc_val: 0.6424 time: 0.5812s\n",
      "Epoch: 0040 loss_train: 0.8952 acc_train: 0.5885 loss_val: 0.8248 acc_val: 0.6493 time: 0.5585s\n",
      "Epoch: 0041 loss_train: 0.8911 acc_train: 0.5920 loss_val: 0.8218 acc_val: 0.6597 time: 0.8995s\n",
      "Epoch: 0042 loss_train: 0.8990 acc_train: 0.5885 loss_val: 0.8185 acc_val: 0.6562 time: 0.6352s\n",
      "Epoch: 0043 loss_train: 0.8953 acc_train: 0.5799 loss_val: 0.8176 acc_val: 0.6701 time: 0.5251s\n",
      "Epoch: 0044 loss_train: 0.8958 acc_train: 0.6372 loss_val: 0.8129 acc_val: 0.6736 time: 0.5873s\n",
      "Epoch: 0045 loss_train: 0.8840 acc_train: 0.6233 loss_val: 0.8072 acc_val: 0.6597 time: 0.5782s\n",
      "Epoch: 0046 loss_train: 0.8746 acc_train: 0.6302 loss_val: 0.8029 acc_val: 0.6632 time: 0.5384s\n",
      "Epoch: 0047 loss_train: 0.8849 acc_train: 0.6111 loss_val: 0.8016 acc_val: 0.6840 time: 0.5498s\n",
      "Epoch: 0048 loss_train: 0.8794 acc_train: 0.6128 loss_val: 0.8024 acc_val: 0.6806 time: 0.6969s\n",
      "Epoch: 0049 loss_train: 0.8706 acc_train: 0.6267 loss_val: 0.8044 acc_val: 0.6667 time: 0.5809s\n",
      "Epoch: 0050 loss_train: 0.8661 acc_train: 0.6233 loss_val: 0.8080 acc_val: 0.6667 time: 0.4728s\n",
      "Epoch: 0051 loss_train: 0.8789 acc_train: 0.6285 loss_val: 0.8025 acc_val: 0.6597 time: 0.4906s\n",
      "Epoch: 0052 loss_train: 0.8677 acc_train: 0.6128 loss_val: 0.7981 acc_val: 0.6736 time: 0.5462s\n",
      "Epoch: 0053 loss_train: 0.8699 acc_train: 0.6233 loss_val: 0.7975 acc_val: 0.6701 time: 0.5084s\n",
      "Epoch: 0054 loss_train: 0.8689 acc_train: 0.6198 loss_val: 0.7975 acc_val: 0.6632 time: 0.5539s\n",
      "Epoch: 0055 loss_train: 0.8641 acc_train: 0.6233 loss_val: 0.8002 acc_val: 0.6667 time: 0.4899s\n",
      "Epoch: 0056 loss_train: 0.8683 acc_train: 0.6181 loss_val: 0.8022 acc_val: 0.6701 time: 0.5193s\n",
      "Epoch: 0057 loss_train: 0.8643 acc_train: 0.6250 loss_val: 0.8001 acc_val: 0.6736 time: 0.7731s\n",
      "Epoch: 0058 loss_train: 0.8732 acc_train: 0.6233 loss_val: 0.7925 acc_val: 0.6632 time: 0.4842s\n",
      "Epoch: 0059 loss_train: 0.8692 acc_train: 0.6181 loss_val: 0.7888 acc_val: 0.6840 time: 0.4453s\n",
      "Epoch: 0060 loss_train: 0.8608 acc_train: 0.6128 loss_val: 0.7876 acc_val: 0.6875 time: 0.4715s\n",
      "Epoch: 0061 loss_train: 0.8701 acc_train: 0.6181 loss_val: 0.7882 acc_val: 0.6736 time: 0.5053s\n",
      "Epoch: 0062 loss_train: 0.8725 acc_train: 0.6250 loss_val: 0.7905 acc_val: 0.6632 time: 0.5043s\n",
      "Epoch: 0063 loss_train: 0.8606 acc_train: 0.6354 loss_val: 0.7933 acc_val: 0.6701 time: 0.4971s\n",
      "Epoch: 0064 loss_train: 0.8628 acc_train: 0.6267 loss_val: 0.7951 acc_val: 0.6771 time: 0.6905s\n",
      "Epoch: 0065 loss_train: 0.8499 acc_train: 0.6250 loss_val: 0.7946 acc_val: 0.6736 time: 0.5835s\n",
      "Epoch: 0066 loss_train: 0.8477 acc_train: 0.6441 loss_val: 0.7909 acc_val: 0.6597 time: 0.5300s\n",
      "Epoch: 0067 loss_train: 0.8529 acc_train: 0.6250 loss_val: 0.7891 acc_val: 0.6667 time: 0.5430s\n",
      "Epoch: 0068 loss_train: 0.8591 acc_train: 0.6337 loss_val: 0.7863 acc_val: 0.6875 time: 0.7296s\n",
      "Epoch: 0069 loss_train: 0.8456 acc_train: 0.6302 loss_val: 0.7850 acc_val: 0.6806 time: 0.5537s\n",
      "Epoch: 0070 loss_train: 0.8524 acc_train: 0.6302 loss_val: 0.7847 acc_val: 0.6806 time: 0.5138s\n",
      "Epoch: 0071 loss_train: 0.8556 acc_train: 0.6337 loss_val: 0.7864 acc_val: 0.6701 time: 0.4985s\n",
      "Epoch: 0072 loss_train: 0.8534 acc_train: 0.6250 loss_val: 0.7926 acc_val: 0.6632 time: 0.5239s\n",
      "Epoch: 0073 loss_train: 0.8623 acc_train: 0.6406 loss_val: 0.7952 acc_val: 0.6736 time: 0.7566s\n",
      "Epoch: 0074 loss_train: 0.8528 acc_train: 0.6250 loss_val: 0.7942 acc_val: 0.6701 time: 0.5010s\n",
      "Epoch: 0075 loss_train: 0.8474 acc_train: 0.6337 loss_val: 0.7902 acc_val: 0.6632 time: 0.5175s\n",
      "Epoch: 0076 loss_train: 0.8529 acc_train: 0.6319 loss_val: 0.7875 acc_val: 0.6736 time: 0.4859s\n",
      "Epoch: 0077 loss_train: 0.8545 acc_train: 0.6233 loss_val: 0.7853 acc_val: 0.6840 time: 0.4726s\n",
      "Epoch: 0078 loss_train: 0.8433 acc_train: 0.6285 loss_val: 0.7831 acc_val: 0.6840 time: 0.5289s\n",
      "Epoch: 0079 loss_train: 0.8482 acc_train: 0.6337 loss_val: 0.7815 acc_val: 0.6840 time: 0.5289s\n",
      "Epoch: 0080 loss_train: 0.8587 acc_train: 0.6267 loss_val: 0.7816 acc_val: 0.6806 time: 0.5263s\n",
      "Epoch: 0081 loss_train: 0.8507 acc_train: 0.6354 loss_val: 0.7827 acc_val: 0.6840 time: 0.6060s\n",
      "Epoch: 0082 loss_train: 0.8467 acc_train: 0.6267 loss_val: 0.7854 acc_val: 0.6771 time: 0.7810s\n",
      "Epoch: 0083 loss_train: 0.8552 acc_train: 0.6302 loss_val: 0.7870 acc_val: 0.6632 time: 0.5200s\n",
      "Epoch: 0084 loss_train: 0.8440 acc_train: 0.6337 loss_val: 0.7860 acc_val: 0.6736 time: 0.4900s\n",
      "Epoch: 0085 loss_train: 0.8502 acc_train: 0.6198 loss_val: 0.7850 acc_val: 0.6771 time: 0.5617s\n",
      "Epoch: 0086 loss_train: 0.8455 acc_train: 0.6215 loss_val: 0.7817 acc_val: 0.6806 time: 0.5160s\n",
      "Epoch: 0087 loss_train: 0.8522 acc_train: 0.6302 loss_val: 0.7790 acc_val: 0.6875 time: 0.5146s\n",
      "Epoch: 0088 loss_train: 0.8406 acc_train: 0.6250 loss_val: 0.7790 acc_val: 0.6875 time: 0.6156s\n",
      "Epoch: 0089 loss_train: 0.8430 acc_train: 0.6319 loss_val: 0.7804 acc_val: 0.6875 time: 0.5260s\n",
      "Epoch: 0090 loss_train: 0.8376 acc_train: 0.6389 loss_val: 0.7821 acc_val: 0.6875 time: 0.5732s\n",
      "Epoch: 0091 loss_train: 0.8323 acc_train: 0.6372 loss_val: 0.7858 acc_val: 0.6701 time: 0.5334s\n",
      "Epoch: 0092 loss_train: 0.8357 acc_train: 0.6354 loss_val: 0.7877 acc_val: 0.6701 time: 0.5424s\n",
      "Epoch: 0093 loss_train: 0.8405 acc_train: 0.6458 loss_val: 0.7819 acc_val: 0.6701 time: 0.5554s\n",
      "Epoch: 0094 loss_train: 0.8385 acc_train: 0.6389 loss_val: 0.7750 acc_val: 0.6840 time: 0.5314s\n",
      "Epoch: 0095 loss_train: 0.8423 acc_train: 0.6302 loss_val: 0.7733 acc_val: 0.6944 time: 0.5333s\n",
      "Epoch: 0096 loss_train: 0.8542 acc_train: 0.6215 loss_val: 0.7742 acc_val: 0.6875 time: 0.6724s\n",
      "Epoch: 0097 loss_train: 0.8410 acc_train: 0.6285 loss_val: 0.7796 acc_val: 0.6736 time: 0.5103s\n",
      "Epoch: 0098 loss_train: 0.8375 acc_train: 0.6250 loss_val: 0.7904 acc_val: 0.6771 time: 0.4826s\n",
      "Epoch: 0099 loss_train: 0.8447 acc_train: 0.6354 loss_val: 0.7968 acc_val: 0.6736 time: 0.4725s\n",
      "Epoch: 0100 loss_train: 0.8483 acc_train: 0.6493 loss_val: 0.7878 acc_val: 0.6806 time: 0.5113s\n",
      "Epoch: 0101 loss_train: 0.8316 acc_train: 0.6441 loss_val: 0.7791 acc_val: 0.6701 time: 0.5224s\n",
      "Epoch: 0102 loss_train: 0.8346 acc_train: 0.6372 loss_val: 0.7745 acc_val: 0.6875 time: 0.4918s\n",
      "Epoch: 0103 loss_train: 0.8410 acc_train: 0.6319 loss_val: 0.7736 acc_val: 0.6910 time: 0.4719s\n",
      "Epoch: 0104 loss_train: 0.8343 acc_train: 0.6302 loss_val: 0.7728 acc_val: 0.6910 time: 0.4748s\n",
      "Epoch: 0105 loss_train: 0.8338 acc_train: 0.6406 loss_val: 0.7731 acc_val: 0.6910 time: 0.4852s\n",
      "Epoch: 0106 loss_train: 0.8335 acc_train: 0.6372 loss_val: 0.7726 acc_val: 0.6875 time: 0.5024s\n",
      "Epoch: 0107 loss_train: 0.8315 acc_train: 0.6424 loss_val: 0.7717 acc_val: 0.6910 time: 0.4993s\n",
      "Epoch: 0108 loss_train: 0.8261 acc_train: 0.6372 loss_val: 0.7724 acc_val: 0.6840 time: 0.4803s\n",
      "Epoch: 0109 loss_train: 0.8400 acc_train: 0.6319 loss_val: 0.7739 acc_val: 0.6875 time: 0.4818s\n",
      "Epoch: 0110 loss_train: 0.8251 acc_train: 0.6406 loss_val: 0.7754 acc_val: 0.6771 time: 0.4832s\n",
      "Epoch: 0111 loss_train: 0.8347 acc_train: 0.6389 loss_val: 0.7739 acc_val: 0.6806 time: 0.4700s\n",
      "Epoch: 0112 loss_train: 0.8357 acc_train: 0.6406 loss_val: 0.7712 acc_val: 0.6840 time: 0.4809s\n",
      "Epoch: 0113 loss_train: 0.8324 acc_train: 0.6267 loss_val: 0.7707 acc_val: 0.6840 time: 0.4815s\n",
      "Epoch: 0114 loss_train: 0.8291 acc_train: 0.6337 loss_val: 0.7737 acc_val: 0.6771 time: 0.4818s\n",
      "Epoch: 0115 loss_train: 0.8291 acc_train: 0.6406 loss_val: 0.7754 acc_val: 0.6806 time: 0.4860s\n",
      "Epoch: 0116 loss_train: 0.8205 acc_train: 0.6389 loss_val: 0.7774 acc_val: 0.6806 time: 0.5134s\n",
      "Epoch: 0117 loss_train: 0.8291 acc_train: 0.6337 loss_val: 0.7754 acc_val: 0.6806 time: 0.4841s\n",
      "Epoch: 0118 loss_train: 0.8235 acc_train: 0.6389 loss_val: 0.7699 acc_val: 0.6910 time: 0.4714s\n",
      "Epoch: 0119 loss_train: 0.8233 acc_train: 0.6302 loss_val: 0.7681 acc_val: 0.6944 time: 0.4595s\n",
      "Epoch: 0120 loss_train: 0.8305 acc_train: 0.6302 loss_val: 0.7695 acc_val: 0.6910 time: 0.4559s\n",
      "Epoch: 0121 loss_train: 0.8237 acc_train: 0.6372 loss_val: 0.7758 acc_val: 0.6806 time: 0.4587s\n",
      "Epoch: 0122 loss_train: 0.8233 acc_train: 0.6372 loss_val: 0.7807 acc_val: 0.6840 time: 0.4614s\n",
      "Epoch: 0123 loss_train: 0.8254 acc_train: 0.6354 loss_val: 0.7737 acc_val: 0.6771 time: 0.4597s\n",
      "Epoch: 0124 loss_train: 0.8220 acc_train: 0.6319 loss_val: 0.7670 acc_val: 0.6944 time: 0.4633s\n",
      "Epoch: 0125 loss_train: 0.8137 acc_train: 0.6406 loss_val: 0.7645 acc_val: 0.6910 time: 0.4519s\n",
      "Epoch: 0126 loss_train: 0.8269 acc_train: 0.6250 loss_val: 0.7645 acc_val: 0.6875 time: 0.4593s\n",
      "Epoch: 0127 loss_train: 0.8207 acc_train: 0.6163 loss_val: 0.7697 acc_val: 0.6840 time: 0.4514s\n",
      "Epoch: 0128 loss_train: 0.8227 acc_train: 0.6337 loss_val: 0.7876 acc_val: 0.6701 time: 0.4564s\n",
      "Epoch: 0129 loss_train: 0.8239 acc_train: 0.6389 loss_val: 0.8059 acc_val: 0.6597 time: 0.4930s\n",
      "Epoch: 0130 loss_train: 0.8340 acc_train: 0.6337 loss_val: 0.7825 acc_val: 0.6806 time: 0.4873s\n",
      "Epoch: 0131 loss_train: 0.8255 acc_train: 0.6476 loss_val: 0.7649 acc_val: 0.6944 time: 0.4850s\n",
      "Epoch: 0132 loss_train: 0.8163 acc_train: 0.6354 loss_val: 0.7693 acc_val: 0.6701 time: 0.4796s\n",
      "Epoch: 0133 loss_train: 0.8358 acc_train: 0.6389 loss_val: 0.7701 acc_val: 0.6597 time: 0.4868s\n",
      "Epoch: 0134 loss_train: 0.8415 acc_train: 0.6285 loss_val: 0.7640 acc_val: 0.6875 time: 0.4916s\n",
      "Epoch: 0135 loss_train: 0.8192 acc_train: 0.6337 loss_val: 0.7742 acc_val: 0.6806 time: 0.4747s\n",
      "Epoch: 0136 loss_train: 0.8188 acc_train: 0.6493 loss_val: 0.7904 acc_val: 0.6736 time: 0.4840s\n",
      "Epoch: 0137 loss_train: 0.8240 acc_train: 0.6476 loss_val: 0.7925 acc_val: 0.6667 time: 0.4993s\n",
      "Epoch: 0138 loss_train: 0.8313 acc_train: 0.6389 loss_val: 0.7725 acc_val: 0.6806 time: 0.4980s\n",
      "Epoch: 0139 loss_train: 0.8132 acc_train: 0.6441 loss_val: 0.7623 acc_val: 0.6910 time: 0.4936s\n",
      "Epoch: 0140 loss_train: 0.8137 acc_train: 0.6389 loss_val: 0.7653 acc_val: 0.6736 time: 0.4721s\n",
      "Epoch: 0141 loss_train: 0.8241 acc_train: 0.6198 loss_val: 0.7670 acc_val: 0.6701 time: 0.4799s\n",
      "Epoch: 0142 loss_train: 0.8419 acc_train: 0.6111 loss_val: 0.7627 acc_val: 0.6944 time: 0.5005s\n",
      "Epoch: 0143 loss_train: 0.8221 acc_train: 0.6354 loss_val: 0.7691 acc_val: 0.6771 time: 0.4963s\n",
      "Epoch: 0144 loss_train: 0.8175 acc_train: 0.6441 loss_val: 0.7895 acc_val: 0.6736 time: 0.4982s\n",
      "Epoch: 0145 loss_train: 0.8316 acc_train: 0.6441 loss_val: 0.7942 acc_val: 0.6597 time: 0.4973s\n",
      "Epoch: 0146 loss_train: 0.8320 acc_train: 0.6458 loss_val: 0.7812 acc_val: 0.6771 time: 0.4891s\n",
      "Epoch: 0147 loss_train: 0.8159 acc_train: 0.6458 loss_val: 0.7696 acc_val: 0.6771 time: 0.4970s\n",
      "Epoch: 0148 loss_train: 0.8116 acc_train: 0.6319 loss_val: 0.7639 acc_val: 0.6910 time: 0.5020s\n",
      "Epoch: 0149 loss_train: 0.8142 acc_train: 0.6302 loss_val: 0.7624 acc_val: 0.6944 time: 0.4995s\n",
      "Epoch: 0150 loss_train: 0.8253 acc_train: 0.6285 loss_val: 0.7630 acc_val: 0.6910 time: 0.5007s\n",
      "Epoch: 0151 loss_train: 0.8243 acc_train: 0.6354 loss_val: 0.7665 acc_val: 0.6736 time: 0.4962s\n",
      "Epoch: 0152 loss_train: 0.8214 acc_train: 0.6354 loss_val: 0.7712 acc_val: 0.6840 time: 0.5842s\n",
      "Epoch: 0153 loss_train: 0.8157 acc_train: 0.6389 loss_val: 0.7732 acc_val: 0.6840 time: 0.5011s\n",
      "Epoch: 0154 loss_train: 0.8122 acc_train: 0.6458 loss_val: 0.7697 acc_val: 0.6771 time: 0.5004s\n",
      "Epoch: 0155 loss_train: 0.8191 acc_train: 0.6354 loss_val: 0.7683 acc_val: 0.6771 time: 0.5086s\n",
      "Epoch: 0156 loss_train: 0.8140 acc_train: 0.6441 loss_val: 0.7655 acc_val: 0.6910 time: 0.4844s\n",
      "Epoch: 0157 loss_train: 0.8166 acc_train: 0.6285 loss_val: 0.7652 acc_val: 0.6910 time: 0.4667s\n",
      "Epoch: 0158 loss_train: 0.8107 acc_train: 0.6441 loss_val: 0.7665 acc_val: 0.6875 time: 0.4691s\n",
      "Epoch: 0159 loss_train: 0.8143 acc_train: 0.6476 loss_val: 0.7672 acc_val: 0.6840 time: 0.4640s\n",
      "Epoch: 0160 loss_train: 0.8122 acc_train: 0.6372 loss_val: 0.7691 acc_val: 0.6736 time: 0.4587s\n",
      "Epoch: 0161 loss_train: 0.8147 acc_train: 0.6476 loss_val: 0.7718 acc_val: 0.6771 time: 0.4745s\n",
      "Epoch: 0162 loss_train: 0.8106 acc_train: 0.6528 loss_val: 0.7699 acc_val: 0.6771 time: 0.4725s\n",
      "Epoch: 0163 loss_train: 0.8114 acc_train: 0.6476 loss_val: 0.7658 acc_val: 0.6840 time: 0.4586s\n",
      "Epoch: 0164 loss_train: 0.8111 acc_train: 0.6476 loss_val: 0.7628 acc_val: 0.6944 time: 0.6987s\n",
      "Epoch: 0165 loss_train: 0.8119 acc_train: 0.6302 loss_val: 0.7623 acc_val: 0.6944 time: 0.4826s\n",
      "Epoch: 0166 loss_train: 0.8133 acc_train: 0.6319 loss_val: 0.7637 acc_val: 0.6910 time: 0.6575s\n",
      "Epoch: 0167 loss_train: 0.8072 acc_train: 0.6354 loss_val: 0.7672 acc_val: 0.6736 time: 0.4599s\n",
      "Epoch: 0168 loss_train: 0.8138 acc_train: 0.6354 loss_val: 0.7674 acc_val: 0.6736 time: 0.4532s\n",
      "Epoch: 0169 loss_train: 0.8148 acc_train: 0.6354 loss_val: 0.7673 acc_val: 0.6771 time: 0.4663s\n",
      "Epoch: 0170 loss_train: 0.8187 acc_train: 0.6302 loss_val: 0.7673 acc_val: 0.6771 time: 0.4646s\n",
      "Epoch: 0171 loss_train: 0.8116 acc_train: 0.6302 loss_val: 0.7651 acc_val: 0.6736 time: 0.4870s\n",
      "Epoch: 0172 loss_train: 0.8077 acc_train: 0.6424 loss_val: 0.7644 acc_val: 0.6806 time: 0.4850s\n",
      "Epoch: 0173 loss_train: 0.8017 acc_train: 0.6493 loss_val: 0.7643 acc_val: 0.6806 time: 0.4774s\n",
      "Epoch: 0174 loss_train: 0.8079 acc_train: 0.6337 loss_val: 0.7648 acc_val: 0.6736 time: 0.4656s\n",
      "Epoch: 0175 loss_train: 0.8097 acc_train: 0.6493 loss_val: 0.7643 acc_val: 0.6806 time: 0.4800s\n",
      "Epoch: 0176 loss_train: 0.8072 acc_train: 0.6354 loss_val: 0.7664 acc_val: 0.6736 time: 0.4557s\n",
      "Epoch: 0177 loss_train: 0.8002 acc_train: 0.6441 loss_val: 0.7682 acc_val: 0.6771 time: 0.4557s\n",
      "Epoch: 0178 loss_train: 0.8060 acc_train: 0.6493 loss_val: 0.7688 acc_val: 0.6771 time: 0.4682s\n",
      "Epoch: 0179 loss_train: 0.8098 acc_train: 0.6372 loss_val: 0.7671 acc_val: 0.6736 time: 0.4525s\n",
      "Epoch: 0180 loss_train: 0.8010 acc_train: 0.6372 loss_val: 0.7653 acc_val: 0.6771 time: 0.4650s\n",
      "Epoch: 0181 loss_train: 0.8114 acc_train: 0.6372 loss_val: 0.7630 acc_val: 0.6910 time: 0.4737s\n",
      "Epoch: 0182 loss_train: 0.8092 acc_train: 0.6458 loss_val: 0.7619 acc_val: 0.6910 time: 0.4497s\n",
      "Epoch: 0183 loss_train: 0.8091 acc_train: 0.6302 loss_val: 0.7624 acc_val: 0.6910 time: 0.4750s\n",
      "Epoch: 0184 loss_train: 0.8062 acc_train: 0.6372 loss_val: 0.7641 acc_val: 0.6806 time: 0.4772s\n",
      "Epoch: 0185 loss_train: 0.8067 acc_train: 0.6389 loss_val: 0.7680 acc_val: 0.6771 time: 0.4663s\n",
      "Epoch: 0186 loss_train: 0.8067 acc_train: 0.6424 loss_val: 0.7677 acc_val: 0.6771 time: 0.4786s\n",
      "Epoch: 0187 loss_train: 0.8001 acc_train: 0.6493 loss_val: 0.7661 acc_val: 0.6771 time: 0.4640s\n",
      "Epoch: 0188 loss_train: 0.8039 acc_train: 0.6476 loss_val: 0.7643 acc_val: 0.6806 time: 0.4551s\n",
      "Epoch: 0189 loss_train: 0.8019 acc_train: 0.6372 loss_val: 0.7629 acc_val: 0.6875 time: 0.4751s\n",
      "Epoch: 0190 loss_train: 0.8082 acc_train: 0.6406 loss_val: 0.7618 acc_val: 0.6910 time: 0.4664s\n",
      "Epoch: 0191 loss_train: 0.8082 acc_train: 0.6441 loss_val: 0.7630 acc_val: 0.6840 time: 0.4639s\n",
      "Epoch: 0192 loss_train: 0.8095 acc_train: 0.6302 loss_val: 0.7667 acc_val: 0.6736 time: 0.4776s\n",
      "Epoch: 0193 loss_train: 0.8059 acc_train: 0.6389 loss_val: 0.7762 acc_val: 0.6736 time: 0.4808s\n",
      "Epoch: 0194 loss_train: 0.8137 acc_train: 0.6458 loss_val: 0.7710 acc_val: 0.6806 time: 0.4985s\n",
      "Epoch: 0195 loss_train: 0.8051 acc_train: 0.6493 loss_val: 0.7625 acc_val: 0.6840 time: 0.4735s\n",
      "Epoch: 0196 loss_train: 0.8076 acc_train: 0.6389 loss_val: 0.7598 acc_val: 0.6944 time: 0.4669s\n",
      "Epoch: 0197 loss_train: 0.8184 acc_train: 0.6233 loss_val: 0.7595 acc_val: 0.6979 time: 0.4868s\n",
      "Epoch: 0198 loss_train: 0.8107 acc_train: 0.6337 loss_val: 0.7617 acc_val: 0.6875 time: 0.4617s\n",
      "Epoch: 0199 loss_train: 0.8031 acc_train: 0.6372 loss_val: 0.7761 acc_val: 0.6701 time: 0.4875s\n",
      "Epoch: 0200 loss_train: 0.8071 acc_train: 0.6372 loss_val: 0.7928 acc_val: 0.6493 time: 0.4615s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 103.1439s\n",
      "Test set results: loss= 0.7928 accuracy= 0.6493\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from pygcn.utils import load_data, accuracy\n",
    "from pygcn.models import GCN\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--fastmode', action='store_true', default=False,\n",
    "                    help='Validate during training pass.')\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=200,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=16,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "\n",
    "args = parser.parse_args(['--epochs=200','--dropout=0.5','--hidden=16','--weight_decay=5e-4','--seed=2349039492328'])\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=args.hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=args.dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFNN control - only consider self values\n",
    "adj = torch.eye(adj.size(0)).to_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use original affinity network. Do not clip small values\n",
    "# slow and not good\n",
    "adj = sparse_tensor_from_arr(rowNormalize(aff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "adj = sparse_tensor_from_arr(rowNormalize(aff_clip))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Findings\n",
    "\n",
    "When dropout is 0, using adj matrix is better than identity matrix\n",
    "\n",
    "But if dropout is 0.5 or 0.9, using identity matrix is better than adj matrix"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
