{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "# os.chdir('/home/jiageng/Documents/fhr/pipeline/')\n",
    "os.chdir('/home/jiageng/Documents/fhr/pygcn/pygcn')\n",
    "import snf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdNormalize(df):\n",
    "    std = df.std().fillna(1)\n",
    "    mean = df - df.mean()\n",
    "    df_norm = mean / std\n",
    "    return np.array(df_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowNormalize(mx):\n",
    "    \"\"\"Row-normalize matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = np.diag(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "def sparse_tensor_from_arr(arr):\n",
    "    \"\"\"Convert numpy array to torch sparse tensor\"\"\"\n",
    "    # Convert numpy array to scipy sparse matrix\n",
    "    sparse_sp = coo_matrix(arr)\n",
    "    \n",
    "    # Convert scipy sparse matrix to torch sparse tensor\n",
    "    sparse_tensor = torch.sparse_coo_tensor(\n",
    "        torch.LongTensor([sparse_sp.row, sparse_sp.col]),\n",
    "        torch.FloatTensor(sparse_sp.data),\n",
    "        torch.Size(sparse_sp.shape)\n",
    "    )\n",
    "    \n",
    "    return sparse_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.read_csv('/home/jiageng/Documents/fhr/annotations/fhr-annotations.tsv',sep='\\t').set_index('PUBLIC_ID').query('risk != -1')\n",
    "df_labels['risk'] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to samples with fhr labels\n",
    "data_full = pd.read_csv('/home/jiageng/Documents/fhr/matrices/gene_mut_matrix_gt1.tsv.gz',sep='\\t').set_index('PUBLIC_ID')\n",
    "if 'SAMPLE' in data_full.columns:\n",
    "    data_full = data_full.drop(columns=['SAMPLE'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(974, 10)\n"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv('/home/jiageng/Documents/fhr/matrices/gene_mut_matrix_fdr5e2.tsv',sep='\\t').set_index('PUBLIC_ID')\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use all samples with features, handle the missing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "974\n"
     ]
    }
   ],
   "source": [
    "# use all samples\n",
    "public_ids = data.index\n",
    "print(len(public_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([974, 10])\n"
     ]
    }
   ],
   "source": [
    "feature_norm_mtd = 'stdnorm' \n",
    "\n",
    "if feature_norm_mtd == 'stdnorm':\n",
    "    features = torch.tensor(stdNormalize(data.loc[public_ids]),dtype=torch.float32)\n",
    "elif feature_norm_mtd == 'rownorm':\n",
    "    features = torch.tensor(rowNormalize(data.loc[public_ids].astype(float).values),dtype=torch.float32)\n",
    "else:\n",
    "    # default is to use raw log tpm+1 values\n",
    "    features = torch.tensor(data.loc[public_ids].values,dtype=torch.float32)\n",
    "print(features.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk = df_labels.reindex(pd.Index(public_ids)).loc[public_ids]['risk'].fillna(-1).astype(int)\n",
    "labels = torch.tensor(risk.values,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "risk\n",
       " 1    517\n",
       " 2    259\n",
       "-1    107\n",
       " 3     91\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing values are set to -1\n",
    "risk.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un-labeled 107\n",
      "Labeled train 578\n",
      "Labeled val 289\n"
     ]
    }
   ],
   "source": [
    "idx_labeled = np.where(risk != -1)[0]\n",
    "idx_unlabeled = np.where(risk == -1)[0]\n",
    "idx_val = torch.tensor(idx_labeled[::3])\n",
    "idx_test = idx_val\n",
    "idx_train = torch.tensor([idx for idx in idx_labeled if idx not in idx_val])\n",
    "print('Un-labeled',len(idx_unlabeled))\n",
    "print('Labeled train',len(idx_train))\n",
    "print('Labeled val',len(idx_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "risk\n",
       "1    333\n",
       "2    181\n",
       "3     64\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 = SR, 2 = GHR, 3 = FHR\n",
    "risk.iloc[idx_train].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "risk\n",
       "1    184\n",
       "2     78\n",
       "3     27\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 = SR, 2 = GHR, 3 = FHR\n",
    "# idx_val and idx_test are the same for now\n",
    "risk.iloc[idx_val].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(974, 974)\n",
      "1.7855952252508438\n",
      "0.0004711639034988262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiageng/Documents/fhr/.venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/jiageng/Documents/fhr/.venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import snf\n",
    "aff = snf.make_affinity(stdNormalize(data), metric='euclidean', K=1000, mu=0.5)\n",
    "print(aff.shape)\n",
    "print(aff.max())\n",
    "print(aff.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase the sparsity of the matrix by 0-clipping small values\n",
    "\n",
    "The sparser the matrix, the higher accuracy of the GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95 0.0011096332200402708\n"
     ]
    }
   ],
   "source": [
    "# Method 2 - sparsen after row-normalization\n",
    "# this is the method that works for mutations\n",
    "adj = rowNormalize(aff)\n",
    "pctile=0.95\n",
    "threshold = np.quantile(adj,pctile)\n",
    "print(pctile, threshold)\n",
    "adj[adj < threshold] = 0\n",
    "adj = sparse_tensor_from_arr(adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 404,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.2472 acc_train: 0.3080 loss_val: 1.2084 acc_val: 0.2837 time: 0.0124s\n",
      "Epoch: 0002 loss_train: 1.2141 acc_train: 0.3149 loss_val: 1.1752 acc_val: 0.2837 time: 0.0123s\n",
      "Epoch: 0003 loss_train: 1.1847 acc_train: 0.3149 loss_val: 1.1445 acc_val: 0.2837 time: 0.0073s\n",
      "Epoch: 0004 loss_train: 1.1577 acc_train: 0.3149 loss_val: 1.1156 acc_val: 0.2837 time: 0.0054s\n",
      "Epoch: 0005 loss_train: 1.1326 acc_train: 0.3149 loss_val: 1.0884 acc_val: 0.6955 time: 0.0037s\n",
      "Epoch: 0006 loss_train: 1.1089 acc_train: 0.6315 loss_val: 1.0631 acc_val: 0.6955 time: 0.0051s\n",
      "Epoch: 0007 loss_train: 1.0871 acc_train: 0.6315 loss_val: 1.0397 acc_val: 0.6955 time: 0.0070s\n",
      "Epoch: 0008 loss_train: 1.0670 acc_train: 0.6315 loss_val: 1.0169 acc_val: 0.7059 time: 0.0037s\n",
      "Epoch: 0009 loss_train: 1.0477 acc_train: 0.6436 loss_val: 0.9949 acc_val: 0.7059 time: 0.0039s\n",
      "Epoch: 0010 loss_train: 1.0293 acc_train: 0.6453 loss_val: 0.9740 acc_val: 0.7059 time: 0.0043s\n",
      "Epoch: 0011 loss_train: 1.0121 acc_train: 0.6453 loss_val: 0.9542 acc_val: 0.7059 time: 0.0034s\n",
      "Epoch: 0012 loss_train: 0.9960 acc_train: 0.6453 loss_val: 0.9357 acc_val: 0.7059 time: 0.0048s\n",
      "Epoch: 0013 loss_train: 0.9810 acc_train: 0.6453 loss_val: 0.9184 acc_val: 0.7059 time: 0.0037s\n",
      "Epoch: 0014 loss_train: 0.9673 acc_train: 0.6453 loss_val: 0.9024 acc_val: 0.7059 time: 0.0064s\n",
      "Epoch: 0015 loss_train: 0.9548 acc_train: 0.6453 loss_val: 0.8878 acc_val: 0.7059 time: 0.0045s\n",
      "Epoch: 0016 loss_train: 0.9436 acc_train: 0.6453 loss_val: 0.8746 acc_val: 0.7059 time: 0.0034s\n",
      "Epoch: 0017 loss_train: 0.9336 acc_train: 0.6453 loss_val: 0.8627 acc_val: 0.7059 time: 0.0074s\n",
      "Epoch: 0018 loss_train: 0.9248 acc_train: 0.6453 loss_val: 0.8522 acc_val: 0.7059 time: 0.0044s\n",
      "Epoch: 0019 loss_train: 0.9172 acc_train: 0.6453 loss_val: 0.8429 acc_val: 0.7059 time: 0.0031s\n",
      "Epoch: 0020 loss_train: 0.9106 acc_train: 0.6453 loss_val: 0.8349 acc_val: 0.7059 time: 0.0054s\n",
      "Epoch: 0021 loss_train: 0.9051 acc_train: 0.6453 loss_val: 0.8279 acc_val: 0.7059 time: 0.0054s\n",
      "Epoch: 0022 loss_train: 0.9004 acc_train: 0.6453 loss_val: 0.8217 acc_val: 0.7059 time: 0.0033s\n",
      "Epoch: 0023 loss_train: 0.8964 acc_train: 0.6453 loss_val: 0.8164 acc_val: 0.7059 time: 0.0046s\n",
      "Epoch: 0024 loss_train: 0.8929 acc_train: 0.6453 loss_val: 0.8119 acc_val: 0.7059 time: 0.0068s\n",
      "Epoch: 0025 loss_train: 0.8898 acc_train: 0.6453 loss_val: 0.8080 acc_val: 0.7059 time: 0.0036s\n",
      "Epoch: 0026 loss_train: 0.8870 acc_train: 0.6453 loss_val: 0.8046 acc_val: 0.7059 time: 0.0039s\n",
      "Epoch: 0027 loss_train: 0.8844 acc_train: 0.6453 loss_val: 0.8018 acc_val: 0.7059 time: 0.0064s\n",
      "Epoch: 0028 loss_train: 0.8820 acc_train: 0.6453 loss_val: 0.7995 acc_val: 0.7059 time: 0.0052s\n",
      "Epoch: 0029 loss_train: 0.8797 acc_train: 0.6453 loss_val: 0.7976 acc_val: 0.7059 time: 0.0030s\n",
      "Epoch: 0030 loss_train: 0.8775 acc_train: 0.6453 loss_val: 0.7962 acc_val: 0.7059 time: 0.0028s\n",
      "Epoch: 0031 loss_train: 0.8753 acc_train: 0.6453 loss_val: 0.7949 acc_val: 0.7059 time: 0.0064s\n",
      "Epoch: 0032 loss_train: 0.8732 acc_train: 0.6453 loss_val: 0.7939 acc_val: 0.7059 time: 0.0042s\n",
      "Epoch: 0033 loss_train: 0.8712 acc_train: 0.6453 loss_val: 0.7931 acc_val: 0.7024 time: 0.0033s\n",
      "Epoch: 0034 loss_train: 0.8692 acc_train: 0.6453 loss_val: 0.7923 acc_val: 0.7024 time: 0.0033s\n",
      "Epoch: 0035 loss_train: 0.8673 acc_train: 0.6453 loss_val: 0.7916 acc_val: 0.7024 time: 0.0068s\n",
      "Epoch: 0036 loss_train: 0.8655 acc_train: 0.6453 loss_val: 0.7908 acc_val: 0.7024 time: 0.0035s\n",
      "Epoch: 0037 loss_train: 0.8636 acc_train: 0.6453 loss_val: 0.7900 acc_val: 0.7024 time: 0.0026s\n",
      "Epoch: 0038 loss_train: 0.8617 acc_train: 0.6453 loss_val: 0.7890 acc_val: 0.7024 time: 0.0046s\n",
      "Epoch: 0039 loss_train: 0.8599 acc_train: 0.6453 loss_val: 0.7880 acc_val: 0.7024 time: 0.0056s\n",
      "Epoch: 0040 loss_train: 0.8581 acc_train: 0.6453 loss_val: 0.7870 acc_val: 0.7024 time: 0.0036s\n",
      "Epoch: 0041 loss_train: 0.8563 acc_train: 0.6453 loss_val: 0.7860 acc_val: 0.7024 time: 0.0033s\n",
      "Epoch: 0042 loss_train: 0.8546 acc_train: 0.6453 loss_val: 0.7851 acc_val: 0.6955 time: 0.0088s\n",
      "Epoch: 0043 loss_train: 0.8530 acc_train: 0.6453 loss_val: 0.7843 acc_val: 0.6955 time: 0.0034s\n",
      "Epoch: 0044 loss_train: 0.8516 acc_train: 0.6453 loss_val: 0.7836 acc_val: 0.6955 time: 0.0026s\n",
      "Epoch: 0045 loss_train: 0.8503 acc_train: 0.6453 loss_val: 0.7831 acc_val: 0.6955 time: 0.0050s\n",
      "Epoch: 0046 loss_train: 0.8491 acc_train: 0.6453 loss_val: 0.7828 acc_val: 0.6955 time: 0.0066s\n",
      "Epoch: 0047 loss_train: 0.8480 acc_train: 0.6453 loss_val: 0.7826 acc_val: 0.7024 time: 0.0035s\n",
      "Epoch: 0048 loss_train: 0.8471 acc_train: 0.6453 loss_val: 0.7826 acc_val: 0.7024 time: 0.0051s\n",
      "Epoch: 0049 loss_train: 0.8462 acc_train: 0.6453 loss_val: 0.7828 acc_val: 0.7024 time: 0.0060s\n",
      "Epoch: 0050 loss_train: 0.8455 acc_train: 0.6453 loss_val: 0.7831 acc_val: 0.7024 time: 0.0034s\n",
      "Epoch: 0051 loss_train: 0.8447 acc_train: 0.6453 loss_val: 0.7835 acc_val: 0.7024 time: 0.0056s\n",
      "Epoch: 0052 loss_train: 0.8441 acc_train: 0.6453 loss_val: 0.7840 acc_val: 0.7024 time: 0.0065s\n",
      "Epoch: 0053 loss_train: 0.8435 acc_train: 0.6453 loss_val: 0.7846 acc_val: 0.7024 time: 0.0047s\n",
      "Epoch: 0054 loss_train: 0.8429 acc_train: 0.6453 loss_val: 0.7851 acc_val: 0.7024 time: 0.0039s\n",
      "Epoch: 0055 loss_train: 0.8424 acc_train: 0.6453 loss_val: 0.7856 acc_val: 0.7024 time: 0.0065s\n",
      "Epoch: 0056 loss_train: 0.8419 acc_train: 0.6453 loss_val: 0.7860 acc_val: 0.7024 time: 0.0036s\n",
      "Epoch: 0057 loss_train: 0.8414 acc_train: 0.6453 loss_val: 0.7864 acc_val: 0.7024 time: 0.0040s\n",
      "Epoch: 0058 loss_train: 0.8409 acc_train: 0.6453 loss_val: 0.7867 acc_val: 0.7024 time: 0.0044s\n",
      "Epoch: 0059 loss_train: 0.8405 acc_train: 0.6453 loss_val: 0.7869 acc_val: 0.7024 time: 0.0067s\n",
      "Epoch: 0060 loss_train: 0.8400 acc_train: 0.6453 loss_val: 0.7871 acc_val: 0.7024 time: 0.0035s\n",
      "Epoch: 0061 loss_train: 0.8395 acc_train: 0.6453 loss_val: 0.7871 acc_val: 0.7024 time: 0.0061s\n",
      "Epoch: 0062 loss_train: 0.8391 acc_train: 0.6453 loss_val: 0.7870 acc_val: 0.7024 time: 0.0065s\n",
      "Epoch: 0063 loss_train: 0.8386 acc_train: 0.6453 loss_val: 0.7869 acc_val: 0.7024 time: 0.0030s\n",
      "Epoch: 0064 loss_train: 0.8382 acc_train: 0.6453 loss_val: 0.7867 acc_val: 0.7024 time: 0.0032s\n",
      "Epoch: 0065 loss_train: 0.8378 acc_train: 0.6453 loss_val: 0.7864 acc_val: 0.7024 time: 0.0083s\n",
      "Epoch: 0066 loss_train: 0.8374 acc_train: 0.6453 loss_val: 0.7861 acc_val: 0.7024 time: 0.0037s\n",
      "Epoch: 0067 loss_train: 0.8370 acc_train: 0.6453 loss_val: 0.7858 acc_val: 0.7024 time: 0.0027s\n",
      "Epoch: 0068 loss_train: 0.8367 acc_train: 0.6453 loss_val: 0.7855 acc_val: 0.6990 time: 0.0073s\n",
      "Epoch: 0069 loss_train: 0.8364 acc_train: 0.6453 loss_val: 0.7852 acc_val: 0.6990 time: 0.0052s\n",
      "Epoch: 0070 loss_train: 0.8361 acc_train: 0.6453 loss_val: 0.7850 acc_val: 0.6990 time: 0.0027s\n",
      "Epoch: 0071 loss_train: 0.8358 acc_train: 0.6453 loss_val: 0.7848 acc_val: 0.6920 time: 0.0043s\n",
      "Epoch: 0072 loss_train: 0.8356 acc_train: 0.6453 loss_val: 0.7847 acc_val: 0.6920 time: 0.0077s\n",
      "Epoch: 0073 loss_train: 0.8353 acc_train: 0.6453 loss_val: 0.7847 acc_val: 0.6920 time: 0.0034s\n",
      "Epoch: 0074 loss_train: 0.8351 acc_train: 0.6453 loss_val: 0.7847 acc_val: 0.6920 time: 0.0032s\n",
      "Epoch: 0075 loss_train: 0.8349 acc_train: 0.6453 loss_val: 0.7848 acc_val: 0.6920 time: 0.0085s\n",
      "Epoch: 0076 loss_train: 0.8347 acc_train: 0.6453 loss_val: 0.7849 acc_val: 0.6990 time: 0.0034s\n",
      "Epoch: 0077 loss_train: 0.8345 acc_train: 0.6453 loss_val: 0.7850 acc_val: 0.6990 time: 0.0026s\n",
      "Epoch: 0078 loss_train: 0.8343 acc_train: 0.6453 loss_val: 0.7851 acc_val: 0.6990 time: 0.0074s\n",
      "Epoch: 0079 loss_train: 0.8342 acc_train: 0.6453 loss_val: 0.7851 acc_val: 0.6990 time: 0.0048s\n",
      "Epoch: 0080 loss_train: 0.8340 acc_train: 0.6453 loss_val: 0.7852 acc_val: 0.6990 time: 0.0026s\n",
      "Epoch: 0081 loss_train: 0.8338 acc_train: 0.6453 loss_val: 0.7852 acc_val: 0.6990 time: 0.0031s\n",
      "Epoch: 0082 loss_train: 0.8337 acc_train: 0.6453 loss_val: 0.7852 acc_val: 0.6990 time: 0.0080s\n",
      "Epoch: 0083 loss_train: 0.8335 acc_train: 0.6453 loss_val: 0.7851 acc_val: 0.6990 time: 0.0043s\n",
      "Epoch: 0084 loss_train: 0.8334 acc_train: 0.6453 loss_val: 0.7851 acc_val: 0.6990 time: 0.0030s\n",
      "Epoch: 0085 loss_train: 0.8333 acc_train: 0.6453 loss_val: 0.7850 acc_val: 0.6990 time: 0.0081s\n",
      "Epoch: 0086 loss_train: 0.8331 acc_train: 0.6453 loss_val: 0.7849 acc_val: 0.6990 time: 0.0047s\n",
      "Epoch: 0087 loss_train: 0.8330 acc_train: 0.6453 loss_val: 0.7849 acc_val: 0.6990 time: 0.0033s\n",
      "Epoch: 0088 loss_train: 0.8329 acc_train: 0.6453 loss_val: 0.7849 acc_val: 0.6990 time: 0.0088s\n",
      "Epoch: 0089 loss_train: 0.8328 acc_train: 0.6453 loss_val: 0.7849 acc_val: 0.6990 time: 0.0050s\n",
      "Epoch: 0090 loss_train: 0.8327 acc_train: 0.6453 loss_val: 0.7849 acc_val: 0.6990 time: 0.0044s\n",
      "Epoch: 0091 loss_train: 0.8326 acc_train: 0.6453 loss_val: 0.7849 acc_val: 0.6990 time: 0.0079s\n",
      "Epoch: 0092 loss_train: 0.8325 acc_train: 0.6453 loss_val: 0.7849 acc_val: 0.6990 time: 0.0035s\n",
      "Epoch: 0093 loss_train: 0.8324 acc_train: 0.6453 loss_val: 0.7850 acc_val: 0.6990 time: 0.0031s\n",
      "Epoch: 0094 loss_train: 0.8323 acc_train: 0.6453 loss_val: 0.7849 acc_val: 0.6990 time: 0.0086s\n",
      "Epoch: 0095 loss_train: 0.8322 acc_train: 0.6453 loss_val: 0.7849 acc_val: 0.6990 time: 0.0035s\n",
      "Epoch: 0096 loss_train: 0.8321 acc_train: 0.6453 loss_val: 0.7848 acc_val: 0.6990 time: 0.0027s\n",
      "Epoch: 0097 loss_train: 0.8320 acc_train: 0.6453 loss_val: 0.7847 acc_val: 0.6990 time: 0.0078s\n",
      "Epoch: 0098 loss_train: 0.8320 acc_train: 0.6453 loss_val: 0.7846 acc_val: 0.6990 time: 0.0047s\n",
      "Epoch: 0099 loss_train: 0.8319 acc_train: 0.6453 loss_val: 0.7844 acc_val: 0.6990 time: 0.0027s\n",
      "Epoch: 0100 loss_train: 0.8318 acc_train: 0.6453 loss_val: 0.7842 acc_val: 0.6990 time: 0.0046s\n",
      "Epoch: 0101 loss_train: 0.8317 acc_train: 0.6453 loss_val: 0.7840 acc_val: 0.6990 time: 0.0073s\n",
      "Epoch: 0102 loss_train: 0.8317 acc_train: 0.6453 loss_val: 0.7838 acc_val: 0.6990 time: 0.0030s\n",
      "Epoch: 0103 loss_train: 0.8316 acc_train: 0.6453 loss_val: 0.7835 acc_val: 0.6990 time: 0.0026s\n",
      "Epoch: 0104 loss_train: 0.8315 acc_train: 0.6453 loss_val: 0.7833 acc_val: 0.6990 time: 0.0080s\n",
      "Epoch: 0105 loss_train: 0.8315 acc_train: 0.6453 loss_val: 0.7831 acc_val: 0.6990 time: 0.0049s\n",
      "Epoch: 0106 loss_train: 0.8314 acc_train: 0.6453 loss_val: 0.7829 acc_val: 0.6990 time: 0.0028s\n",
      "Epoch: 0107 loss_train: 0.8314 acc_train: 0.6453 loss_val: 0.7828 acc_val: 0.6990 time: 0.0102s\n",
      "Epoch: 0108 loss_train: 0.8313 acc_train: 0.6453 loss_val: 0.7827 acc_val: 0.6990 time: 0.0033s\n",
      "Epoch: 0109 loss_train: 0.8312 acc_train: 0.6453 loss_val: 0.7826 acc_val: 0.6990 time: 0.0028s\n",
      "Epoch: 0110 loss_train: 0.8312 acc_train: 0.6453 loss_val: 0.7826 acc_val: 0.6990 time: 0.0085s\n",
      "Epoch: 0111 loss_train: 0.8311 acc_train: 0.6453 loss_val: 0.7825 acc_val: 0.6990 time: 0.0040s\n",
      "Epoch: 0112 loss_train: 0.8311 acc_train: 0.6453 loss_val: 0.7825 acc_val: 0.6990 time: 0.0046s\n",
      "Epoch: 0113 loss_train: 0.8311 acc_train: 0.6453 loss_val: 0.7824 acc_val: 0.6990 time: 0.0108s\n",
      "Epoch: 0114 loss_train: 0.8310 acc_train: 0.6453 loss_val: 0.7823 acc_val: 0.6990 time: 0.0034s\n",
      "Epoch: 0115 loss_train: 0.8310 acc_train: 0.6453 loss_val: 0.7821 acc_val: 0.6990 time: 0.0069s\n",
      "Epoch: 0116 loss_train: 0.8309 acc_train: 0.6453 loss_val: 0.7819 acc_val: 0.6990 time: 0.0069s\n",
      "Epoch: 0117 loss_train: 0.8309 acc_train: 0.6453 loss_val: 0.7817 acc_val: 0.6990 time: 0.0027s\n",
      "Epoch: 0118 loss_train: 0.8308 acc_train: 0.6453 loss_val: 0.7815 acc_val: 0.6990 time: 0.0068s\n",
      "Epoch: 0119 loss_train: 0.8308 acc_train: 0.6453 loss_val: 0.7813 acc_val: 0.6990 time: 0.0052s\n",
      "Epoch: 0120 loss_train: 0.8308 acc_train: 0.6453 loss_val: 0.7811 acc_val: 0.6990 time: 0.0026s\n",
      "Epoch: 0121 loss_train: 0.8307 acc_train: 0.6453 loss_val: 0.7809 acc_val: 0.6990 time: 0.0089s\n",
      "Epoch: 0122 loss_train: 0.8307 acc_train: 0.6453 loss_val: 0.7807 acc_val: 0.6990 time: 0.0060s\n",
      "Epoch: 0123 loss_train: 0.8306 acc_train: 0.6453 loss_val: 0.7806 acc_val: 0.6990 time: 0.0084s\n",
      "Epoch: 0124 loss_train: 0.8306 acc_train: 0.6453 loss_val: 0.7804 acc_val: 0.6990 time: 0.0078s\n",
      "Epoch: 0125 loss_train: 0.8306 acc_train: 0.6453 loss_val: 0.7803 acc_val: 0.6990 time: 0.0075s\n",
      "Epoch: 0126 loss_train: 0.8305 acc_train: 0.6453 loss_val: 0.7802 acc_val: 0.6990 time: 0.0065s\n",
      "Epoch: 0127 loss_train: 0.8305 acc_train: 0.6453 loss_val: 0.7801 acc_val: 0.6990 time: 0.0034s\n",
      "Epoch: 0128 loss_train: 0.8305 acc_train: 0.6453 loss_val: 0.7800 acc_val: 0.6990 time: 0.0066s\n",
      "Epoch: 0129 loss_train: 0.8304 acc_train: 0.6453 loss_val: 0.7799 acc_val: 0.6990 time: 0.0061s\n",
      "Epoch: 0130 loss_train: 0.8304 acc_train: 0.6453 loss_val: 0.7798 acc_val: 0.6990 time: 0.0027s\n",
      "Epoch: 0131 loss_train: 0.8304 acc_train: 0.6453 loss_val: 0.7797 acc_val: 0.6990 time: 0.0046s\n",
      "Epoch: 0132 loss_train: 0.8304 acc_train: 0.6453 loss_val: 0.7795 acc_val: 0.6990 time: 0.0080s\n",
      "Epoch: 0133 loss_train: 0.8303 acc_train: 0.6453 loss_val: 0.7794 acc_val: 0.6990 time: 0.0025s\n",
      "Epoch: 0134 loss_train: 0.8303 acc_train: 0.6453 loss_val: 0.7793 acc_val: 0.6990 time: 0.0022s\n",
      "Epoch: 0135 loss_train: 0.8303 acc_train: 0.6453 loss_val: 0.7791 acc_val: 0.6990 time: 0.0094s\n",
      "Epoch: 0136 loss_train: 0.8302 acc_train: 0.6453 loss_val: 0.7790 acc_val: 0.6990 time: 0.0032s\n",
      "Epoch: 0137 loss_train: 0.8302 acc_train: 0.6453 loss_val: 0.7788 acc_val: 0.6990 time: 0.0024s\n",
      "Epoch: 0138 loss_train: 0.8302 acc_train: 0.6453 loss_val: 0.7787 acc_val: 0.6990 time: 0.0033s\n",
      "Epoch: 0139 loss_train: 0.8302 acc_train: 0.6453 loss_val: 0.7786 acc_val: 0.6990 time: 0.0086s\n",
      "Epoch: 0140 loss_train: 0.8301 acc_train: 0.6453 loss_val: 0.7785 acc_val: 0.6990 time: 0.0026s\n",
      "Epoch: 0141 loss_train: 0.8301 acc_train: 0.6453 loss_val: 0.7784 acc_val: 0.6990 time: 0.0021s\n",
      "Epoch: 0142 loss_train: 0.8301 acc_train: 0.6453 loss_val: 0.7783 acc_val: 0.6990 time: 0.0061s\n",
      "Epoch: 0143 loss_train: 0.8301 acc_train: 0.6453 loss_val: 0.7782 acc_val: 0.6990 time: 0.0055s\n",
      "Epoch: 0144 loss_train: 0.8300 acc_train: 0.6453 loss_val: 0.7781 acc_val: 0.6990 time: 0.0025s\n",
      "Epoch: 0145 loss_train: 0.8300 acc_train: 0.6453 loss_val: 0.7779 acc_val: 0.6990 time: 0.0021s\n",
      "Epoch: 0146 loss_train: 0.8300 acc_train: 0.6453 loss_val: 0.7778 acc_val: 0.6990 time: 0.0082s\n",
      "Epoch: 0147 loss_train: 0.8300 acc_train: 0.6453 loss_val: 0.7777 acc_val: 0.6990 time: 0.0053s\n",
      "Epoch: 0148 loss_train: 0.8299 acc_train: 0.6453 loss_val: 0.7776 acc_val: 0.6990 time: 0.0032s\n",
      "Epoch: 0149 loss_train: 0.8299 acc_train: 0.6453 loss_val: 0.7775 acc_val: 0.6990 time: 0.0086s\n",
      "Epoch: 0150 loss_train: 0.8299 acc_train: 0.6453 loss_val: 0.7774 acc_val: 0.6990 time: 0.0045s\n",
      "Epoch: 0151 loss_train: 0.8299 acc_train: 0.6453 loss_val: 0.7772 acc_val: 0.6990 time: 0.0023s\n",
      "Epoch: 0152 loss_train: 0.8299 acc_train: 0.6453 loss_val: 0.7771 acc_val: 0.6990 time: 0.0076s\n",
      "Epoch: 0153 loss_train: 0.8298 acc_train: 0.6453 loss_val: 0.7770 acc_val: 0.6990 time: 0.0047s\n",
      "Epoch: 0154 loss_train: 0.8298 acc_train: 0.6453 loss_val: 0.7769 acc_val: 0.6990 time: 0.0024s\n",
      "Epoch: 0155 loss_train: 0.8298 acc_train: 0.6453 loss_val: 0.7768 acc_val: 0.6990 time: 0.0021s\n",
      "Epoch: 0156 loss_train: 0.8298 acc_train: 0.6453 loss_val: 0.7767 acc_val: 0.6990 time: 0.0087s\n",
      "Epoch: 0157 loss_train: 0.8298 acc_train: 0.6453 loss_val: 0.7766 acc_val: 0.6990 time: 0.0035s\n",
      "Epoch: 0158 loss_train: 0.8298 acc_train: 0.6453 loss_val: 0.7765 acc_val: 0.6990 time: 0.0023s\n",
      "Epoch: 0159 loss_train: 0.8297 acc_train: 0.6453 loss_val: 0.7764 acc_val: 0.6990 time: 0.0028s\n",
      "Epoch: 0160 loss_train: 0.8297 acc_train: 0.6453 loss_val: 0.7763 acc_val: 0.6990 time: 0.0102s\n",
      "Epoch: 0161 loss_train: 0.8297 acc_train: 0.6453 loss_val: 0.7762 acc_val: 0.6990 time: 0.0032s\n",
      "Epoch: 0162 loss_train: 0.8297 acc_train: 0.6453 loss_val: 0.7761 acc_val: 0.6990 time: 0.0075s\n",
      "Epoch: 0163 loss_train: 0.8297 acc_train: 0.6453 loss_val: 0.7760 acc_val: 0.6990 time: 0.0070s\n",
      "Epoch: 0164 loss_train: 0.8296 acc_train: 0.6453 loss_val: 0.7759 acc_val: 0.6990 time: 0.0052s\n",
      "Epoch: 0165 loss_train: 0.8296 acc_train: 0.6453 loss_val: 0.7758 acc_val: 0.6990 time: 0.0095s\n",
      "Epoch: 0166 loss_train: 0.8296 acc_train: 0.6453 loss_val: 0.7757 acc_val: 0.6990 time: 0.0042s\n",
      "Epoch: 0167 loss_train: 0.8296 acc_train: 0.6453 loss_val: 0.7756 acc_val: 0.6990 time: 0.0103s\n",
      "Epoch: 0168 loss_train: 0.8296 acc_train: 0.6453 loss_val: 0.7755 acc_val: 0.6990 time: 0.0036s\n",
      "Epoch: 0169 loss_train: 0.8296 acc_train: 0.6453 loss_val: 0.7754 acc_val: 0.6990 time: 0.0024s\n",
      "Epoch: 0170 loss_train: 0.8296 acc_train: 0.6453 loss_val: 0.7753 acc_val: 0.6990 time: 0.0095s\n",
      "Epoch: 0171 loss_train: 0.8295 acc_train: 0.6453 loss_val: 0.7752 acc_val: 0.6990 time: 0.0030s\n",
      "Epoch: 0172 loss_train: 0.8295 acc_train: 0.6453 loss_val: 0.7752 acc_val: 0.6990 time: 0.0026s\n",
      "Epoch: 0173 loss_train: 0.8295 acc_train: 0.6453 loss_val: 0.7751 acc_val: 0.6990 time: 0.0075s\n",
      "Epoch: 0174 loss_train: 0.8295 acc_train: 0.6453 loss_val: 0.7750 acc_val: 0.6990 time: 0.0052s\n",
      "Epoch: 0175 loss_train: 0.8295 acc_train: 0.6453 loss_val: 0.7749 acc_val: 0.6990 time: 0.0025s\n",
      "Epoch: 0176 loss_train: 0.8295 acc_train: 0.6453 loss_val: 0.7748 acc_val: 0.6990 time: 0.0031s\n",
      "Epoch: 0177 loss_train: 0.8295 acc_train: 0.6453 loss_val: 0.7747 acc_val: 0.6990 time: 0.0090s\n",
      "Epoch: 0178 loss_train: 0.8294 acc_train: 0.6453 loss_val: 0.7747 acc_val: 0.6990 time: 0.0028s\n",
      "Epoch: 0179 loss_train: 0.8294 acc_train: 0.6453 loss_val: 0.7746 acc_val: 0.6990 time: 0.0023s\n",
      "Epoch: 0180 loss_train: 0.8294 acc_train: 0.6453 loss_val: 0.7745 acc_val: 0.6990 time: 0.0064s\n",
      "Epoch: 0181 loss_train: 0.8294 acc_train: 0.6453 loss_val: 0.7744 acc_val: 0.6990 time: 0.0055s\n",
      "Epoch: 0182 loss_train: 0.8294 acc_train: 0.6453 loss_val: 0.7743 acc_val: 0.6990 time: 0.0025s\n",
      "Epoch: 0183 loss_train: 0.8294 acc_train: 0.6453 loss_val: 0.7742 acc_val: 0.6990 time: 0.0023s\n",
      "Epoch: 0184 loss_train: 0.8294 acc_train: 0.6453 loss_val: 0.7742 acc_val: 0.6990 time: 0.0092s\n",
      "Epoch: 0185 loss_train: 0.8294 acc_train: 0.6453 loss_val: 0.7741 acc_val: 0.6990 time: 0.0040s\n",
      "Epoch: 0186 loss_train: 0.8293 acc_train: 0.6453 loss_val: 0.7740 acc_val: 0.6990 time: 0.0042s\n",
      "Epoch: 0187 loss_train: 0.8293 acc_train: 0.6453 loss_val: 0.7739 acc_val: 0.6990 time: 0.0090s\n",
      "Epoch: 0188 loss_train: 0.8293 acc_train: 0.6453 loss_val: 0.7739 acc_val: 0.6990 time: 0.0028s\n",
      "Epoch: 0189 loss_train: 0.8293 acc_train: 0.6453 loss_val: 0.7738 acc_val: 0.6990 time: 0.0023s\n",
      "Epoch: 0190 loss_train: 0.8293 acc_train: 0.6453 loss_val: 0.7737 acc_val: 0.6990 time: 0.0085s\n",
      "Epoch: 0191 loss_train: 0.8293 acc_train: 0.6453 loss_val: 0.7737 acc_val: 0.6990 time: 0.0040s\n",
      "Epoch: 0192 loss_train: 0.8293 acc_train: 0.6453 loss_val: 0.7736 acc_val: 0.6990 time: 0.0028s\n",
      "Epoch: 0193 loss_train: 0.8293 acc_train: 0.6453 loss_val: 0.7735 acc_val: 0.6990 time: 0.0045s\n",
      "Epoch: 0194 loss_train: 0.8292 acc_train: 0.6453 loss_val: 0.7735 acc_val: 0.6990 time: 0.0077s\n",
      "Epoch: 0195 loss_train: 0.8292 acc_train: 0.6453 loss_val: 0.7734 acc_val: 0.6990 time: 0.0025s\n",
      "Epoch: 0196 loss_train: 0.8292 acc_train: 0.6453 loss_val: 0.7733 acc_val: 0.6990 time: 0.0024s\n",
      "Epoch: 0197 loss_train: 0.8292 acc_train: 0.6453 loss_val: 0.7733 acc_val: 0.6990 time: 0.0071s\n",
      "Epoch: 0198 loss_train: 0.8292 acc_train: 0.6453 loss_val: 0.7732 acc_val: 0.6990 time: 0.0049s\n",
      "Epoch: 0199 loss_train: 0.8292 acc_train: 0.6453 loss_val: 0.7732 acc_val: 0.6990 time: 0.0035s\n",
      "Epoch: 0200 loss_train: 0.8292 acc_train: 0.6453 loss_val: 0.7731 acc_val: 0.6990 time: 0.0064s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 1.0517s\n",
      "Test set results: loss= 0.7731 accuracy= 0.6990\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from pygcn.utils import load_data, accuracy\n",
    "from pygcn.models import GCN\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--fastmode', action='store_true', default=False,\n",
    "                    help='Validate during training pass.')\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=200,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=16,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "\n",
    "args = parser.parse_args(['--epochs=200','--dropout=0.','--hidden=16','--weight_decay=5e-4','--seed=1023420948521123'])\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=args.hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=args.dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "metadata": {},
   "outputs": [],
   "source": [
    "# negative control - only consider self values\n",
    "adj = torch.eye(adj.size(0)).to_sparse()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For 85 genes (chi2 p < 0.05 and freq >= 10), less TP53\n",
    "\n",
    "Feature matrix should be row-normalized\n",
    "\n",
    "Affinity network needs high K e.g. K=1000, because in case neighbours are exactly the same\n",
    "\n",
    "For gene mutation matrix, a low percentile filter for sparsity e.g. 0.5 is better.\n",
    "\n",
    "Also, the sparsity filter should only be applied after row normalization\n",
    "\n",
    "Accuracy is slightly above 0.7.  Using identity adjacent matrix cannot get this high.\n",
    "\n",
    "Very high dropout of 0.9-0.95 is needed, otherwise model will overfit to training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## For 10 genes (FDR 0.05)\n",
    "\n",
    "Accuracy of identity adjacency matrix is consistently above 0.7 when dropout is 0.5. \n",
    "\n",
    "Accuracy is very slightly above 0.7 but is not any better \n",
    "\n",
    "Conclusion 10 features is too few for the benefits of GCN to be realised"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
