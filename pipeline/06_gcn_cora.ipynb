{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test out pygcn using the default CORA dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/jiageng/Documents/fhr/pygcn/pygcn')\n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from pygcn.utils import load_data, accuracy\n",
    "from pygcn.models import GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--fastmode', action='store_true', default=False,\n",
    "                    help='Validate during training pass.')\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=200,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=16,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "\n",
    "args = parser.parse_args(['--fastmode'])\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cora dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiageng/Documents/fhr/.venv/lib/python3.12/site-packages/pygcn-0.1-py3.12.egg/pygcn/utils.py:80: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:644.)\n"
     ]
    }
   ],
   "source": [
    "adj, features, labels, idx_train, idx_val, idx_test = load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace sparse tensor adj with an identity matrix of the same shape\n",
    "adj_rand = torch.eye(adj.size(0)).to_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=args.hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=args.dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj_rand)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj_rand)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj_rand)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 1.9851 acc_train: 0.0929 loss_val: 1.9899 acc_val: 0.1100 time: 0.0409s\n",
      "Epoch: 0002 loss_train: 1.9749 acc_train: 0.0929 loss_val: 1.9819 acc_val: 0.1067 time: 0.0325s\n",
      "Epoch: 0003 loss_train: 1.9723 acc_train: 0.1000 loss_val: 1.9751 acc_val: 0.1000 time: 0.0332s\n",
      "Epoch: 0004 loss_train: 1.9548 acc_train: 0.1071 loss_val: 1.9545 acc_val: 0.1133 time: 0.0281s\n",
      "Epoch: 0005 loss_train: 1.9425 acc_train: 0.1500 loss_val: 1.9405 acc_val: 0.1033 time: 0.0285s\n",
      "Epoch: 0006 loss_train: 1.9259 acc_train: 0.1214 loss_val: 1.9301 acc_val: 0.1433 time: 0.0251s\n",
      "Epoch: 0007 loss_train: 1.9067 acc_train: 0.1786 loss_val: 1.9231 acc_val: 0.1067 time: 0.0184s\n",
      "Epoch: 0008 loss_train: 1.9065 acc_train: 0.1714 loss_val: 1.9097 acc_val: 0.1333 time: 0.0151s\n",
      "Epoch: 0009 loss_train: 1.8738 acc_train: 0.2357 loss_val: 1.8991 acc_val: 0.1833 time: 0.0208s\n",
      "Epoch: 0010 loss_train: 1.8665 acc_train: 0.2429 loss_val: 1.8933 acc_val: 0.2200 time: 0.0176s\n",
      "Epoch: 0011 loss_train: 1.8529 acc_train: 0.2571 loss_val: 1.8670 acc_val: 0.2400 time: 0.0167s\n",
      "Epoch: 0012 loss_train: 1.8303 acc_train: 0.2929 loss_val: 1.8675 acc_val: 0.2733 time: 0.0174s\n",
      "Epoch: 0013 loss_train: 1.8016 acc_train: 0.3429 loss_val: 1.8622 acc_val: 0.2467 time: 0.0170s\n",
      "Epoch: 0014 loss_train: 1.8035 acc_train: 0.3143 loss_val: 1.8405 acc_val: 0.3100 time: 0.0181s\n",
      "Epoch: 0015 loss_train: 1.7855 acc_train: 0.3714 loss_val: 1.8394 acc_val: 0.2833 time: 0.0245s\n",
      "Epoch: 0016 loss_train: 1.7729 acc_train: 0.3214 loss_val: 1.8254 acc_val: 0.3167 time: 0.0164s\n",
      "Epoch: 0017 loss_train: 1.7127 acc_train: 0.4000 loss_val: 1.8235 acc_val: 0.3200 time: 0.0176s\n",
      "Epoch: 0018 loss_train: 1.7574 acc_train: 0.3286 loss_val: 1.7938 acc_val: 0.3133 time: 0.0155s\n",
      "Epoch: 0019 loss_train: 1.7329 acc_train: 0.3357 loss_val: 1.8040 acc_val: 0.3433 time: 0.0182s\n",
      "Epoch: 0020 loss_train: 1.7303 acc_train: 0.3857 loss_val: 1.8221 acc_val: 0.3233 time: 0.0164s\n",
      "Epoch: 0021 loss_train: 1.6802 acc_train: 0.3714 loss_val: 1.7977 acc_val: 0.3167 time: 0.0154s\n",
      "Epoch: 0022 loss_train: 1.6875 acc_train: 0.3857 loss_val: 1.7928 acc_val: 0.3467 time: 0.0143s\n",
      "Epoch: 0023 loss_train: 1.6532 acc_train: 0.4214 loss_val: 1.8061 acc_val: 0.3333 time: 0.0182s\n",
      "Epoch: 0024 loss_train: 1.6437 acc_train: 0.4000 loss_val: 1.7656 acc_val: 0.3300 time: 0.0191s\n",
      "Epoch: 0025 loss_train: 1.6547 acc_train: 0.4000 loss_val: 1.7579 acc_val: 0.3400 time: 0.0242s\n",
      "Epoch: 0026 loss_train: 1.6341 acc_train: 0.4214 loss_val: 1.7853 acc_val: 0.3567 time: 0.0170s\n",
      "Epoch: 0027 loss_train: 1.6283 acc_train: 0.4071 loss_val: 1.7937 acc_val: 0.3233 time: 0.0134s\n",
      "Epoch: 0028 loss_train: 1.5901 acc_train: 0.4214 loss_val: 1.7733 acc_val: 0.3467 time: 0.0147s\n",
      "Epoch: 0029 loss_train: 1.6347 acc_train: 0.4143 loss_val: 1.7608 acc_val: 0.3633 time: 0.0166s\n",
      "Epoch: 0030 loss_train: 1.5811 acc_train: 0.4071 loss_val: 1.7799 acc_val: 0.3200 time: 0.0151s\n",
      "Epoch: 0031 loss_train: 1.5834 acc_train: 0.4286 loss_val: 1.7820 acc_val: 0.3567 time: 0.0213s\n",
      "Epoch: 0032 loss_train: 1.5732 acc_train: 0.4143 loss_val: 1.7672 acc_val: 0.3667 time: 0.0177s\n",
      "Epoch: 0033 loss_train: 1.5538 acc_train: 0.4500 loss_val: 1.7494 acc_val: 0.3533 time: 0.0142s\n",
      "Epoch: 0034 loss_train: 1.5610 acc_train: 0.4143 loss_val: 1.7447 acc_val: 0.3467 time: 0.0167s\n",
      "Epoch: 0035 loss_train: 1.5348 acc_train: 0.3857 loss_val: 1.7491 acc_val: 0.3767 time: 0.0183s\n",
      "Epoch: 0036 loss_train: 1.5141 acc_train: 0.4214 loss_val: 1.7306 acc_val: 0.3800 time: 0.0355s\n",
      "Epoch: 0037 loss_train: 1.4351 acc_train: 0.4571 loss_val: 1.7433 acc_val: 0.3600 time: 0.0225s\n",
      "Epoch: 0038 loss_train: 1.4370 acc_train: 0.5000 loss_val: 1.7352 acc_val: 0.3833 time: 0.0237s\n",
      "Epoch: 0039 loss_train: 1.3912 acc_train: 0.5071 loss_val: 1.7344 acc_val: 0.3433 time: 0.0244s\n",
      "Epoch: 0040 loss_train: 1.4297 acc_train: 0.5000 loss_val: 1.7323 acc_val: 0.3467 time: 0.0237s\n",
      "Epoch: 0041 loss_train: 1.4019 acc_train: 0.5000 loss_val: 1.7036 acc_val: 0.3867 time: 0.0178s\n",
      "Epoch: 0042 loss_train: 1.3403 acc_train: 0.5143 loss_val: 1.7154 acc_val: 0.3767 time: 0.0170s\n",
      "Epoch: 0043 loss_train: 1.3509 acc_train: 0.5643 loss_val: 1.6913 acc_val: 0.3833 time: 0.0210s\n",
      "Epoch: 0044 loss_train: 1.3168 acc_train: 0.5286 loss_val: 1.6923 acc_val: 0.3833 time: 0.0200s\n",
      "Epoch: 0045 loss_train: 1.3183 acc_train: 0.5214 loss_val: 1.6966 acc_val: 0.3633 time: 0.0173s\n",
      "Epoch: 0046 loss_train: 1.3098 acc_train: 0.6214 loss_val: 1.6837 acc_val: 0.3833 time: 0.0181s\n",
      "Epoch: 0047 loss_train: 1.2774 acc_train: 0.6000 loss_val: 1.6860 acc_val: 0.3400 time: 0.0312s\n",
      "Epoch: 0048 loss_train: 1.2973 acc_train: 0.5714 loss_val: 1.6566 acc_val: 0.3733 time: 0.0256s\n",
      "Epoch: 0049 loss_train: 1.2661 acc_train: 0.6214 loss_val: 1.6542 acc_val: 0.3733 time: 0.0231s\n",
      "Epoch: 0050 loss_train: 1.2806 acc_train: 0.5643 loss_val: 1.6446 acc_val: 0.3967 time: 0.0290s\n",
      "Epoch: 0051 loss_train: 1.2243 acc_train: 0.6143 loss_val: 1.6317 acc_val: 0.4133 time: 0.0237s\n",
      "Epoch: 0052 loss_train: 1.2173 acc_train: 0.5714 loss_val: 1.6301 acc_val: 0.4033 time: 0.0325s\n",
      "Epoch: 0053 loss_train: 1.1952 acc_train: 0.6429 loss_val: 1.6353 acc_val: 0.4167 time: 0.0278s\n",
      "Epoch: 0054 loss_train: 1.2068 acc_train: 0.5571 loss_val: 1.6624 acc_val: 0.3600 time: 0.0266s\n",
      "Epoch: 0055 loss_train: 1.1137 acc_train: 0.6500 loss_val: 1.6452 acc_val: 0.3667 time: 0.0245s\n",
      "Epoch: 0056 loss_train: 1.0903 acc_train: 0.6643 loss_val: 1.6566 acc_val: 0.3767 time: 0.0191s\n",
      "Epoch: 0057 loss_train: 1.1071 acc_train: 0.6214 loss_val: 1.6708 acc_val: 0.3800 time: 0.0279s\n",
      "Epoch: 0058 loss_train: 1.1303 acc_train: 0.6643 loss_val: 1.6280 acc_val: 0.3967 time: 0.0202s\n",
      "Epoch: 0059 loss_train: 1.1071 acc_train: 0.6643 loss_val: 1.6262 acc_val: 0.4067 time: 0.0177s\n",
      "Epoch: 0060 loss_train: 1.0205 acc_train: 0.6571 loss_val: 1.6254 acc_val: 0.4200 time: 0.0200s\n",
      "Epoch: 0061 loss_train: 1.0557 acc_train: 0.6786 loss_val: 1.5909 acc_val: 0.4100 time: 0.0242s\n",
      "Epoch: 0062 loss_train: 1.0690 acc_train: 0.5857 loss_val: 1.6236 acc_val: 0.3967 time: 0.0226s\n",
      "Epoch: 0063 loss_train: 1.0916 acc_train: 0.6571 loss_val: 1.6154 acc_val: 0.3967 time: 0.0279s\n",
      "Epoch: 0064 loss_train: 0.9756 acc_train: 0.6857 loss_val: 1.5742 acc_val: 0.4100 time: 0.0244s\n",
      "Epoch: 0065 loss_train: 1.0411 acc_train: 0.6643 loss_val: 1.6148 acc_val: 0.3800 time: 0.0264s\n",
      "Epoch: 0066 loss_train: 0.9524 acc_train: 0.7000 loss_val: 1.5765 acc_val: 0.4500 time: 0.0274s\n",
      "Epoch: 0067 loss_train: 0.9100 acc_train: 0.7643 loss_val: 1.6049 acc_val: 0.4000 time: 0.0233s\n",
      "Epoch: 0068 loss_train: 0.9340 acc_train: 0.7357 loss_val: 1.5844 acc_val: 0.4167 time: 0.0261s\n",
      "Epoch: 0069 loss_train: 0.9032 acc_train: 0.7143 loss_val: 1.6346 acc_val: 0.3867 time: 0.0237s\n",
      "Epoch: 0070 loss_train: 0.9192 acc_train: 0.7071 loss_val: 1.5948 acc_val: 0.4267 time: 0.0260s\n",
      "Epoch: 0071 loss_train: 0.9350 acc_train: 0.7357 loss_val: 1.5685 acc_val: 0.4100 time: 0.0251s\n",
      "Epoch: 0072 loss_train: 0.9116 acc_train: 0.6714 loss_val: 1.5852 acc_val: 0.4000 time: 0.0280s\n",
      "Epoch: 0073 loss_train: 0.7938 acc_train: 0.7929 loss_val: 1.6042 acc_val: 0.4100 time: 0.0192s\n",
      "Epoch: 0074 loss_train: 0.8698 acc_train: 0.7357 loss_val: 1.5613 acc_val: 0.4300 time: 0.0264s\n",
      "Epoch: 0075 loss_train: 0.8719 acc_train: 0.7286 loss_val: 1.5882 acc_val: 0.4100 time: 0.0218s\n",
      "Epoch: 0076 loss_train: 0.7734 acc_train: 0.7643 loss_val: 1.5404 acc_val: 0.4300 time: 0.0181s\n",
      "Epoch: 0077 loss_train: 0.7715 acc_train: 0.7929 loss_val: 1.5718 acc_val: 0.3967 time: 0.0223s\n",
      "Epoch: 0078 loss_train: 0.7826 acc_train: 0.7714 loss_val: 1.5308 acc_val: 0.4300 time: 0.0248s\n",
      "Epoch: 0079 loss_train: 0.8261 acc_train: 0.7286 loss_val: 1.5387 acc_val: 0.4533 time: 0.0193s\n",
      "Epoch: 0080 loss_train: 0.8394 acc_train: 0.7214 loss_val: 1.5751 acc_val: 0.3867 time: 0.0279s\n",
      "Epoch: 0081 loss_train: 0.8031 acc_train: 0.7643 loss_val: 1.6019 acc_val: 0.3900 time: 0.0251s\n",
      "Epoch: 0082 loss_train: 0.7250 acc_train: 0.7929 loss_val: 1.5634 acc_val: 0.4333 time: 0.0277s\n",
      "Epoch: 0083 loss_train: 0.7390 acc_train: 0.7929 loss_val: 1.5215 acc_val: 0.4300 time: 0.0181s\n",
      "Epoch: 0084 loss_train: 0.6977 acc_train: 0.8071 loss_val: 1.4844 acc_val: 0.4567 time: 0.0217s\n",
      "Epoch: 0085 loss_train: 0.7137 acc_train: 0.8071 loss_val: 1.5248 acc_val: 0.4333 time: 0.0223s\n",
      "Epoch: 0086 loss_train: 0.7686 acc_train: 0.7714 loss_val: 1.5620 acc_val: 0.3967 time: 0.0211s\n",
      "Epoch: 0087 loss_train: 0.6828 acc_train: 0.7857 loss_val: 1.5790 acc_val: 0.3900 time: 0.0264s\n",
      "Epoch: 0088 loss_train: 0.7046 acc_train: 0.7929 loss_val: 1.5294 acc_val: 0.4367 time: 0.0225s\n",
      "Epoch: 0089 loss_train: 0.7341 acc_train: 0.7643 loss_val: 1.4797 acc_val: 0.4833 time: 0.0277s\n",
      "Epoch: 0090 loss_train: 0.7413 acc_train: 0.7429 loss_val: 1.5823 acc_val: 0.4033 time: 0.0257s\n",
      "Epoch: 0091 loss_train: 0.7856 acc_train: 0.7500 loss_val: 1.5308 acc_val: 0.4100 time: 0.0286s\n",
      "Epoch: 0092 loss_train: 0.6994 acc_train: 0.8071 loss_val: 1.4733 acc_val: 0.4667 time: 0.0194s\n",
      "Epoch: 0093 loss_train: 0.6715 acc_train: 0.8071 loss_val: 1.4988 acc_val: 0.4400 time: 0.0244s\n",
      "Epoch: 0094 loss_train: 0.6365 acc_train: 0.8071 loss_val: 1.5084 acc_val: 0.4500 time: 0.0222s\n",
      "Epoch: 0095 loss_train: 0.6617 acc_train: 0.8071 loss_val: 1.5500 acc_val: 0.3933 time: 0.0248s\n",
      "Epoch: 0096 loss_train: 0.6000 acc_train: 0.8286 loss_val: 1.4641 acc_val: 0.4400 time: 0.0225s\n",
      "Epoch: 0097 loss_train: 0.6555 acc_train: 0.7929 loss_val: 1.5334 acc_val: 0.4433 time: 0.0200s\n",
      "Epoch: 0098 loss_train: 0.7195 acc_train: 0.7857 loss_val: 1.5312 acc_val: 0.4167 time: 0.0281s\n",
      "Epoch: 0099 loss_train: 0.6069 acc_train: 0.8357 loss_val: 1.5484 acc_val: 0.4267 time: 0.0328s\n",
      "Epoch: 0100 loss_train: 0.6104 acc_train: 0.8143 loss_val: 1.5026 acc_val: 0.4533 time: 0.0283s\n",
      "Epoch: 0101 loss_train: 0.7160 acc_train: 0.7643 loss_val: 1.4249 acc_val: 0.4867 time: 0.0242s\n",
      "Epoch: 0102 loss_train: 0.6739 acc_train: 0.8286 loss_val: 1.4852 acc_val: 0.4633 time: 0.0227s\n",
      "Epoch: 0103 loss_train: 0.5769 acc_train: 0.8429 loss_val: 1.5319 acc_val: 0.4500 time: 0.0268s\n",
      "Epoch: 0104 loss_train: 0.6481 acc_train: 0.8357 loss_val: 1.4976 acc_val: 0.4300 time: 0.0259s\n",
      "Epoch: 0105 loss_train: 0.5991 acc_train: 0.8357 loss_val: 1.4844 acc_val: 0.4533 time: 0.0249s\n",
      "Epoch: 0106 loss_train: 0.6417 acc_train: 0.8286 loss_val: 1.4830 acc_val: 0.4433 time: 0.0280s\n",
      "Epoch: 0107 loss_train: 0.5885 acc_train: 0.8071 loss_val: 1.5342 acc_val: 0.4300 time: 0.0231s\n",
      "Epoch: 0108 loss_train: 0.5758 acc_train: 0.8571 loss_val: 1.5786 acc_val: 0.4267 time: 0.0285s\n",
      "Epoch: 0109 loss_train: 0.5876 acc_train: 0.8286 loss_val: 1.5102 acc_val: 0.4333 time: 0.0216s\n",
      "Epoch: 0110 loss_train: 0.5444 acc_train: 0.8714 loss_val: 1.5189 acc_val: 0.4667 time: 0.0260s\n",
      "Epoch: 0111 loss_train: 0.5393 acc_train: 0.8643 loss_val: 1.4703 acc_val: 0.4600 time: 0.0229s\n",
      "Epoch: 0112 loss_train: 0.6007 acc_train: 0.8143 loss_val: 1.5082 acc_val: 0.4567 time: 0.0269s\n",
      "Epoch: 0113 loss_train: 0.5854 acc_train: 0.8071 loss_val: 1.4999 acc_val: 0.4533 time: 0.0212s\n",
      "Epoch: 0114 loss_train: 0.5864 acc_train: 0.8571 loss_val: 1.5106 acc_val: 0.4567 time: 0.0256s\n",
      "Epoch: 0115 loss_train: 0.5988 acc_train: 0.8071 loss_val: 1.5198 acc_val: 0.4533 time: 0.0224s\n",
      "Epoch: 0116 loss_train: 0.5238 acc_train: 0.8571 loss_val: 1.5032 acc_val: 0.4433 time: 0.0278s\n",
      "Epoch: 0117 loss_train: 0.5456 acc_train: 0.8571 loss_val: 1.5026 acc_val: 0.4667 time: 0.0251s\n",
      "Epoch: 0118 loss_train: 0.5755 acc_train: 0.8357 loss_val: 1.5236 acc_val: 0.4567 time: 0.0278s\n",
      "Epoch: 0119 loss_train: 0.5326 acc_train: 0.8571 loss_val: 1.5037 acc_val: 0.4633 time: 0.0237s\n",
      "Epoch: 0120 loss_train: 0.5559 acc_train: 0.8286 loss_val: 1.5187 acc_val: 0.4600 time: 0.0269s\n",
      "Epoch: 0121 loss_train: 0.5388 acc_train: 0.8786 loss_val: 1.4725 acc_val: 0.4567 time: 0.0236s\n",
      "Epoch: 0122 loss_train: 0.5418 acc_train: 0.8500 loss_val: 1.5347 acc_val: 0.4133 time: 0.0255s\n",
      "Epoch: 0123 loss_train: 0.4966 acc_train: 0.8500 loss_val: 1.5023 acc_val: 0.4367 time: 0.0260s\n",
      "Epoch: 0124 loss_train: 0.4902 acc_train: 0.8500 loss_val: 1.4892 acc_val: 0.4400 time: 0.0229s\n",
      "Epoch: 0125 loss_train: 0.5092 acc_train: 0.8786 loss_val: 1.5681 acc_val: 0.4067 time: 0.0268s\n",
      "Epoch: 0126 loss_train: 0.5611 acc_train: 0.8429 loss_val: 1.5059 acc_val: 0.4300 time: 0.0237s\n",
      "Epoch: 0127 loss_train: 0.5838 acc_train: 0.7786 loss_val: 1.4866 acc_val: 0.4700 time: 0.0259s\n",
      "Epoch: 0128 loss_train: 0.5979 acc_train: 0.8000 loss_val: 1.4858 acc_val: 0.4333 time: 0.0240s\n",
      "Epoch: 0129 loss_train: 0.5020 acc_train: 0.8857 loss_val: 1.5132 acc_val: 0.4667 time: 0.0230s\n",
      "Epoch: 0130 loss_train: 0.5411 acc_train: 0.8643 loss_val: 1.5221 acc_val: 0.4700 time: 0.0256s\n",
      "Epoch: 0131 loss_train: 0.5249 acc_train: 0.8714 loss_val: 1.5169 acc_val: 0.4567 time: 0.0249s\n",
      "Epoch: 0132 loss_train: 0.5288 acc_train: 0.8429 loss_val: 1.5306 acc_val: 0.4433 time: 0.0260s\n",
      "Epoch: 0133 loss_train: 0.5184 acc_train: 0.8500 loss_val: 1.5232 acc_val: 0.4500 time: 0.0269s\n",
      "Epoch: 0134 loss_train: 0.5043 acc_train: 0.8143 loss_val: 1.4619 acc_val: 0.4467 time: 0.0229s\n",
      "Epoch: 0135 loss_train: 0.4890 acc_train: 0.8714 loss_val: 1.5090 acc_val: 0.4633 time: 0.0258s\n",
      "Epoch: 0136 loss_train: 0.4797 acc_train: 0.8714 loss_val: 1.5021 acc_val: 0.4433 time: 0.0228s\n",
      "Epoch: 0137 loss_train: 0.5596 acc_train: 0.7929 loss_val: 1.6004 acc_val: 0.4100 time: 0.0236s\n",
      "Epoch: 0138 loss_train: 0.5872 acc_train: 0.7929 loss_val: 1.5177 acc_val: 0.4400 time: 0.0251s\n",
      "Epoch: 0139 loss_train: 0.4505 acc_train: 0.8643 loss_val: 1.5132 acc_val: 0.4400 time: 0.0251s\n",
      "Epoch: 0140 loss_train: 0.5255 acc_train: 0.8286 loss_val: 1.4832 acc_val: 0.4700 time: 0.0251s\n",
      "Epoch: 0141 loss_train: 0.4484 acc_train: 0.8571 loss_val: 1.5779 acc_val: 0.4167 time: 0.0260s\n",
      "Epoch: 0142 loss_train: 0.5979 acc_train: 0.8000 loss_val: 1.4718 acc_val: 0.4667 time: 0.0252s\n",
      "Epoch: 0143 loss_train: 0.4833 acc_train: 0.8857 loss_val: 1.4785 acc_val: 0.4733 time: 0.0285s\n",
      "Epoch: 0144 loss_train: 0.5454 acc_train: 0.8571 loss_val: 1.5456 acc_val: 0.4233 time: 0.0218s\n",
      "Epoch: 0145 loss_train: 0.4534 acc_train: 0.8643 loss_val: 1.4976 acc_val: 0.4267 time: 0.0267s\n",
      "Epoch: 0146 loss_train: 0.4492 acc_train: 0.8786 loss_val: 1.5155 acc_val: 0.4500 time: 0.0216s\n",
      "Epoch: 0147 loss_train: 0.4458 acc_train: 0.8714 loss_val: 1.5312 acc_val: 0.4367 time: 0.0266s\n",
      "Epoch: 0148 loss_train: 0.5166 acc_train: 0.8643 loss_val: 1.4932 acc_val: 0.4400 time: 0.0231s\n",
      "Epoch: 0149 loss_train: 0.4847 acc_train: 0.8429 loss_val: 1.4837 acc_val: 0.4600 time: 0.0278s\n",
      "Epoch: 0150 loss_train: 0.4357 acc_train: 0.9000 loss_val: 1.5201 acc_val: 0.4200 time: 0.0228s\n",
      "Epoch: 0151 loss_train: 0.4776 acc_train: 0.8571 loss_val: 1.4509 acc_val: 0.4700 time: 0.0211s\n",
      "Epoch: 0152 loss_train: 0.4459 acc_train: 0.9071 loss_val: 1.5744 acc_val: 0.4300 time: 0.0195s\n",
      "Epoch: 0153 loss_train: 0.4077 acc_train: 0.9143 loss_val: 1.4993 acc_val: 0.4600 time: 0.0180s\n",
      "Epoch: 0154 loss_train: 0.5150 acc_train: 0.8357 loss_val: 1.5137 acc_val: 0.4400 time: 0.0195s\n",
      "Epoch: 0155 loss_train: 0.4757 acc_train: 0.8786 loss_val: 1.5137 acc_val: 0.4200 time: 0.0174s\n",
      "Epoch: 0156 loss_train: 0.5541 acc_train: 0.8214 loss_val: 1.5381 acc_val: 0.4367 time: 0.0173s\n",
      "Epoch: 0157 loss_train: 0.5136 acc_train: 0.8714 loss_val: 1.4978 acc_val: 0.4633 time: 0.0252s\n",
      "Epoch: 0158 loss_train: 0.4820 acc_train: 0.8500 loss_val: 1.4701 acc_val: 0.4667 time: 0.0230s\n",
      "Epoch: 0159 loss_train: 0.4552 acc_train: 0.8786 loss_val: 1.5241 acc_val: 0.4533 time: 0.0198s\n",
      "Epoch: 0160 loss_train: 0.4373 acc_train: 0.8857 loss_val: 1.5000 acc_val: 0.4433 time: 0.0270s\n",
      "Epoch: 0161 loss_train: 0.4084 acc_train: 0.9143 loss_val: 1.5507 acc_val: 0.4500 time: 0.0186s\n",
      "Epoch: 0162 loss_train: 0.4467 acc_train: 0.8643 loss_val: 1.5093 acc_val: 0.4600 time: 0.0222s\n",
      "Epoch: 0163 loss_train: 0.5377 acc_train: 0.8143 loss_val: 1.5284 acc_val: 0.4433 time: 0.0303s\n",
      "Epoch: 0164 loss_train: 0.4877 acc_train: 0.8429 loss_val: 1.5368 acc_val: 0.4333 time: 0.0306s\n",
      "Epoch: 0165 loss_train: 0.4427 acc_train: 0.8571 loss_val: 1.5537 acc_val: 0.4600 time: 0.0282s\n",
      "Epoch: 0166 loss_train: 0.4300 acc_train: 0.9071 loss_val: 1.4787 acc_val: 0.4567 time: 0.0206s\n",
      "Epoch: 0167 loss_train: 0.4772 acc_train: 0.8214 loss_val: 1.5971 acc_val: 0.4200 time: 0.0167s\n",
      "Epoch: 0168 loss_train: 0.4178 acc_train: 0.8786 loss_val: 1.6008 acc_val: 0.4500 time: 0.0191s\n",
      "Epoch: 0169 loss_train: 0.3812 acc_train: 0.9071 loss_val: 1.4705 acc_val: 0.4700 time: 0.0277s\n",
      "Epoch: 0170 loss_train: 0.4589 acc_train: 0.8786 loss_val: 1.5289 acc_val: 0.4367 time: 0.0244s\n",
      "Epoch: 0171 loss_train: 0.4471 acc_train: 0.8714 loss_val: 1.4772 acc_val: 0.4333 time: 0.0290s\n",
      "Epoch: 0172 loss_train: 0.5319 acc_train: 0.7857 loss_val: 1.5268 acc_val: 0.4433 time: 0.0251s\n",
      "Epoch: 0173 loss_train: 0.4284 acc_train: 0.8857 loss_val: 1.5057 acc_val: 0.4467 time: 0.0238s\n",
      "Epoch: 0174 loss_train: 0.4600 acc_train: 0.8714 loss_val: 1.5140 acc_val: 0.4433 time: 0.0232s\n",
      "Epoch: 0175 loss_train: 0.4258 acc_train: 0.8643 loss_val: 1.5093 acc_val: 0.4733 time: 0.0252s\n",
      "Epoch: 0176 loss_train: 0.4227 acc_train: 0.8714 loss_val: 1.5306 acc_val: 0.4767 time: 0.0184s\n",
      "Epoch: 0177 loss_train: 0.4440 acc_train: 0.8857 loss_val: 1.5361 acc_val: 0.4433 time: 0.0231s\n",
      "Epoch: 0178 loss_train: 0.4546 acc_train: 0.8786 loss_val: 1.5127 acc_val: 0.4400 time: 0.0268s\n",
      "Epoch: 0179 loss_train: 0.3902 acc_train: 0.8929 loss_val: 1.4921 acc_val: 0.4733 time: 0.0339s\n",
      "Epoch: 0180 loss_train: 0.3679 acc_train: 0.8929 loss_val: 1.5167 acc_val: 0.4500 time: 0.0262s\n",
      "Epoch: 0181 loss_train: 0.4039 acc_train: 0.8857 loss_val: 1.5567 acc_val: 0.4567 time: 0.0207s\n",
      "Epoch: 0182 loss_train: 0.3900 acc_train: 0.9000 loss_val: 1.5680 acc_val: 0.4067 time: 0.0187s\n",
      "Epoch: 0183 loss_train: 0.4849 acc_train: 0.8357 loss_val: 1.5503 acc_val: 0.4700 time: 0.0289s\n",
      "Epoch: 0184 loss_train: 0.4041 acc_train: 0.8857 loss_val: 1.5633 acc_val: 0.4400 time: 0.0214s\n",
      "Epoch: 0185 loss_train: 0.4133 acc_train: 0.8643 loss_val: 1.5879 acc_val: 0.4433 time: 0.0238s\n",
      "Epoch: 0186 loss_train: 0.4629 acc_train: 0.8357 loss_val: 1.5023 acc_val: 0.4600 time: 0.0241s\n",
      "Epoch: 0187 loss_train: 0.4118 acc_train: 0.8643 loss_val: 1.5625 acc_val: 0.4467 time: 0.0226s\n",
      "Epoch: 0188 loss_train: 0.4045 acc_train: 0.9143 loss_val: 1.4717 acc_val: 0.4833 time: 0.0233s\n",
      "Epoch: 0189 loss_train: 0.4531 acc_train: 0.8714 loss_val: 1.5221 acc_val: 0.4667 time: 0.0268s\n",
      "Epoch: 0190 loss_train: 0.4395 acc_train: 0.8786 loss_val: 1.4511 acc_val: 0.4600 time: 0.0230s\n",
      "Epoch: 0191 loss_train: 0.4424 acc_train: 0.8643 loss_val: 1.4886 acc_val: 0.4733 time: 0.0192s\n",
      "Epoch: 0192 loss_train: 0.3906 acc_train: 0.9071 loss_val: 1.5696 acc_val: 0.4533 time: 0.0267s\n",
      "Epoch: 0193 loss_train: 0.4360 acc_train: 0.8714 loss_val: 1.5698 acc_val: 0.4267 time: 0.0206s\n",
      "Epoch: 0194 loss_train: 0.4383 acc_train: 0.8500 loss_val: 1.5023 acc_val: 0.4633 time: 0.0183s\n",
      "Epoch: 0195 loss_train: 0.4070 acc_train: 0.9071 loss_val: 1.5626 acc_val: 0.4500 time: 0.0262s\n",
      "Epoch: 0196 loss_train: 0.4951 acc_train: 0.8500 loss_val: 1.6234 acc_val: 0.4233 time: 0.0255s\n",
      "Epoch: 0197 loss_train: 0.4226 acc_train: 0.8857 loss_val: 1.5719 acc_val: 0.4167 time: 0.0270s\n",
      "Epoch: 0198 loss_train: 0.4261 acc_train: 0.8500 loss_val: 1.5921 acc_val: 0.4433 time: 0.0175s\n",
      "Epoch: 0199 loss_train: 0.4115 acc_train: 0.8857 loss_val: 1.5147 acc_val: 0.4633 time: 0.0166s\n",
      "Epoch: 0200 loss_train: 0.4041 acc_train: 0.8786 loss_val: 1.5353 acc_val: 0.4633 time: 0.0242s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 4.7229s\n",
      "Test set results: loss= 1.3414 accuracy= 0.5260\n"
     ]
    }
   ],
   "source": [
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
