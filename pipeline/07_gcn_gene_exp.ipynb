{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "# os.chdir('/home/jiageng/Documents/fhr/pipeline/')\n",
    "os.chdir('/home/jiageng/Documents/fhr/pygcn/pygcn')\n",
    "import snf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdNormalize(df):\n",
    "    std = df.std().fillna(1)\n",
    "    mean = df - df.mean()\n",
    "    df_norm = mean / std\n",
    "    return np.array(df_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowNormalize(mx):\n",
    "    \"\"\"Row-normalize matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = np.diag(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "def sparse_tensor_from_arr(arr):\n",
    "    \"\"\"Convert numpy array to torch sparse tensor\"\"\"\n",
    "    # Convert numpy array to scipy sparse matrix\n",
    "    sparse_sp = coo_matrix(arr)\n",
    "    \n",
    "    # Convert scipy sparse matrix to torch sparse tensor\n",
    "    sparse_tensor = torch.sparse_coo_tensor(\n",
    "        torch.LongTensor([sparse_sp.row, sparse_sp.col]),\n",
    "        torch.FloatTensor(sparse_sp.data),\n",
    "        torch.Size(sparse_sp.shape)\n",
    "    )\n",
    "    \n",
    "    return sparse_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.read_csv('/home/jiageng/Documents/fhr/annotations/fhr-annotations.tsv',sep='\\t').set_index('PUBLIC_ID').query('risk != -1')\n",
    "df_labels['risk'] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(806, 500)\n"
     ]
    }
   ],
   "source": [
    "# subset to samples with fhr labels\n",
    "data = pd.read_csv('/home/jiageng/Documents/fhr/matrices/gene_exp_matrix_k500.tsv',sep='\\t').set_index('PUBLIC_ID')\n",
    "if 'SAMPLE' in data.columns:\n",
    "    data = data.drop(columns=['SAMPLE'])\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use all samples with features, handle the missing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "806\n"
     ]
    }
   ],
   "source": [
    "# use all rna seq samples\n",
    "public_ids = data.index\n",
    "print(len(public_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([806, 500])\n"
     ]
    }
   ],
   "source": [
    "feature_norm_mtd = '' \n",
    "\n",
    "if feature_norm_mtd == 'stdnorm':\n",
    "    features = torch.tensor(stdNormalize(data.loc[public_ids]),dtype=torch.float32)\n",
    "elif feature_norm_mtd == 'rownorm':\n",
    "    features = torch.tensor(rowNormalize(data.loc[public_ids].values),dtype=torch.float32)\n",
    "else:\n",
    "    # default is to use raw log tpm+1 values\n",
    "    features = torch.tensor(data.loc[public_ids].values,dtype=torch.float32)\n",
    "print(features.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk = df_labels.reindex(pd.Index(public_ids)).loc[public_ids]['risk'].fillna(-1).astype(int)\n",
    "labels = torch.tensor(risk.values,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "risk\n",
       " 1    403\n",
       " 2    216\n",
       "-1    105\n",
       " 3     82\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing values are set to -1\n",
    "risk.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un-labeled 105\n",
      "Labeled train 467\n",
      "Labeled val 234\n"
     ]
    }
   ],
   "source": [
    "idx_labeled = np.where(risk != -1)[0]\n",
    "idx_unlabeled = np.where(risk == -1)[0]\n",
    "idx_val = torch.tensor(idx_labeled[::3])\n",
    "idx_test = idx_val\n",
    "idx_train = torch.tensor([idx for idx in idx_labeled if idx not in idx_val])\n",
    "print('Un-labeled',len(idx_unlabeled))\n",
    "print('Labeled train',len(idx_train))\n",
    "print('Labeled val',len(idx_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "risk\n",
       "1    268\n",
       "2    149\n",
       "3     50\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 = SR, 2 = GHR, 3 = FHR\n",
    "risk.iloc[idx_train].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "risk\n",
       "1    135\n",
       "2     67\n",
       "3     32\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 = SR, 2 = GHR, 3 = FHR\n",
    "# idx_val and idx_test are the same for now\n",
    "risk.iloc[idx_val].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiageng/Documents/fhr/.venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/jiageng/Documents/fhr/.venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(806, 806)\n"
     ]
    }
   ],
   "source": [
    "import snf\n",
    "aff = snf.make_affinity(stdNormalize(data), metric='cosine', K=100, mu=0.5)\n",
    "print(aff.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase the sparsity of the matrix by 0-clipping small values\n",
    "\n",
    "The sparser the matrix, the higher accuracy of the GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95 0.18962869481000272\n"
     ]
    }
   ],
   "source": [
    "# Method 1 - sparsen before row-normalization\n",
    "pctile = 0.95\n",
    "threshold = np.quantile(aff,pctile)\n",
    "print(pctile, threshold)\n",
    "aff_clip = aff.copy()\n",
    "aff_clip[aff_clip < threshold] = 0\n",
    "adj = sparse_tensor_from_arr(rowNormalize(aff_clip))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95 0.003963189074209203\n"
     ]
    }
   ],
   "source": [
    "# Method 2 - sparsen after row-normalization\n",
    "# this is the method that works for mutations\n",
    "adj = rowNormalize(aff)\n",
    "pctile=0.95\n",
    "threshold = np.quantile(adj,pctile)\n",
    "print(pctile, threshold)\n",
    "adj[adj < threshold] = 0\n",
    "adj = sparse_tensor_from_arr(adj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 2.3515 acc_train: 0.2637 loss_val: 1.1749 acc_val: 0.3387 time: 0.0238s\n",
      "Epoch: 0002 loss_train: 1.3149 acc_train: 0.3707 loss_val: 1.0689 acc_val: 0.5378 time: 0.0243s\n",
      "Epoch: 0003 loss_train: 1.1371 acc_train: 0.4499 loss_val: 1.1364 acc_val: 0.4386 time: 0.0175s\n",
      "Epoch: 0004 loss_train: 1.5280 acc_train: 0.3279 loss_val: 1.1369 acc_val: 0.4332 time: 0.0150s\n",
      "Epoch: 0005 loss_train: 1.0800 acc_train: 0.4915 loss_val: 1.1072 acc_val: 0.5046 time: 0.0149s\n",
      "Epoch: 0006 loss_train: 1.2009 acc_train: 0.3709 loss_val: 1.0683 acc_val: 0.5586 time: 0.0180s\n",
      "Epoch: 0007 loss_train: 1.0670 acc_train: 0.4779 loss_val: 1.0671 acc_val: 0.4912 time: 0.0153s\n",
      "Epoch: 0008 loss_train: 1.0947 acc_train: 0.4175 loss_val: 1.0671 acc_val: 0.4837 time: 0.0139s\n",
      "Epoch: 0009 loss_train: 1.1208 acc_train: 0.4296 loss_val: 1.0573 acc_val: 0.4788 time: 0.0134s\n",
      "Epoch: 0010 loss_train: 1.1741 acc_train: 0.4143 loss_val: 1.0366 acc_val: 0.5111 time: 0.0122s\n",
      "Epoch: 0011 loss_train: 1.1149 acc_train: 0.4432 loss_val: 1.0183 acc_val: 0.5136 time: 0.0174s\n",
      "Epoch: 0012 loss_train: 1.0795 acc_train: 0.4053 loss_val: 0.9978 acc_val: 0.5211 time: 0.0144s\n",
      "Epoch: 0013 loss_train: 0.9786 acc_train: 0.4808 loss_val: 0.9882 acc_val: 0.5360 time: 0.0148s\n",
      "Epoch: 0014 loss_train: 1.0096 acc_train: 0.4867 loss_val: 0.9604 acc_val: 0.5539 time: 0.0171s\n",
      "Epoch: 0015 loss_train: 1.0155 acc_train: 0.4613 loss_val: 0.9439 acc_val: 0.5539 time: 0.0197s\n",
      "Epoch: 0016 loss_train: 1.0477 acc_train: 0.3944 loss_val: 0.9293 acc_val: 0.5515 time: 0.0187s\n",
      "Epoch: 0017 loss_train: 0.9822 acc_train: 0.4637 loss_val: 0.9164 acc_val: 0.5515 time: 0.0135s\n",
      "Epoch: 0018 loss_train: 0.9145 acc_train: 0.5090 loss_val: 0.9055 acc_val: 0.5515 time: 0.0135s\n",
      "Epoch: 0019 loss_train: 0.9975 acc_train: 0.4607 loss_val: 0.8962 acc_val: 0.5440 time: 0.0164s\n",
      "Epoch: 0020 loss_train: 0.8686 acc_train: 0.4929 loss_val: 0.8885 acc_val: 0.5415 time: 0.0143s\n",
      "Epoch: 0021 loss_train: 0.8845 acc_train: 0.5061 loss_val: 0.8821 acc_val: 0.5315 time: 0.0181s\n",
      "Epoch: 0022 loss_train: 0.8933 acc_train: 0.5204 loss_val: 0.8768 acc_val: 0.5265 time: 0.0189s\n",
      "Epoch: 0023 loss_train: 0.8766 acc_train: 0.5342 loss_val: 0.8724 acc_val: 0.5062 time: 0.0187s\n",
      "Epoch: 0024 loss_train: 0.8875 acc_train: 0.4691 loss_val: 0.8698 acc_val: 0.5161 time: 0.0181s\n",
      "Epoch: 0025 loss_train: 0.8831 acc_train: 0.5106 loss_val: 0.8674 acc_val: 0.4728 time: 0.0236s\n",
      "Epoch: 0026 loss_train: 0.8565 acc_train: 0.4968 loss_val: 0.8715 acc_val: 0.4623 time: 0.0242s\n",
      "Epoch: 0027 loss_train: 0.9016 acc_train: 0.5133 loss_val: 0.8799 acc_val: 0.4415 time: 0.0261s\n",
      "Epoch: 0028 loss_train: 0.8748 acc_train: 0.5153 loss_val: 0.8877 acc_val: 0.4440 time: 0.0182s\n",
      "Epoch: 0029 loss_train: 0.8742 acc_train: 0.4918 loss_val: 0.8897 acc_val: 0.4365 time: 0.0179s\n",
      "Epoch: 0030 loss_train: 0.8899 acc_train: 0.4499 loss_val: 0.8709 acc_val: 0.4489 time: 0.0170s\n",
      "Epoch: 0031 loss_train: 0.8744 acc_train: 0.4940 loss_val: 0.8547 acc_val: 0.4539 time: 0.0192s\n",
      "Epoch: 0032 loss_train: 0.8849 acc_train: 0.4793 loss_val: 0.8513 acc_val: 0.4957 time: 0.0254s\n",
      "Epoch: 0033 loss_train: 0.8942 acc_train: 0.5063 loss_val: 0.8459 acc_val: 0.5106 time: 0.0212s\n",
      "Epoch: 0034 loss_train: 0.9092 acc_train: 0.4822 loss_val: 0.8395 acc_val: 0.4927 time: 0.0268s\n",
      "Epoch: 0035 loss_train: 0.8248 acc_train: 0.4985 loss_val: 0.8310 acc_val: 0.4901 time: 0.0175s\n",
      "Epoch: 0036 loss_train: 0.8673 acc_train: 0.5123 loss_val: 0.8319 acc_val: 0.4752 time: 0.0182s\n",
      "Epoch: 0037 loss_train: 0.8561 acc_train: 0.4868 loss_val: 0.8426 acc_val: 0.4577 time: 0.0229s\n",
      "Epoch: 0038 loss_train: 0.8000 acc_train: 0.4998 loss_val: 0.8508 acc_val: 0.4577 time: 0.0339s\n",
      "Epoch: 0039 loss_train: 0.8964 acc_train: 0.4637 loss_val: 0.8414 acc_val: 0.4577 time: 0.0244s\n",
      "Epoch: 0040 loss_train: 0.8008 acc_train: 0.4974 loss_val: 0.8321 acc_val: 0.4577 time: 0.0201s\n",
      "Epoch: 0041 loss_train: 0.8582 acc_train: 0.4802 loss_val: 0.8205 acc_val: 0.4901 time: 0.0195s\n",
      "Epoch: 0042 loss_train: 0.8269 acc_train: 0.5140 loss_val: 0.8127 acc_val: 0.5050 time: 0.0172s\n",
      "Epoch: 0043 loss_train: 0.9268 acc_train: 0.3945 loss_val: 0.8081 acc_val: 0.5224 time: 0.0188s\n",
      "Epoch: 0044 loss_train: 0.8080 acc_train: 0.5120 loss_val: 0.8062 acc_val: 0.5349 time: 0.0218s\n",
      "Epoch: 0045 loss_train: 0.8284 acc_train: 0.4966 loss_val: 0.8024 acc_val: 0.5324 time: 0.0206s\n",
      "Epoch: 0046 loss_train: 0.8164 acc_train: 0.5155 loss_val: 0.7982 acc_val: 0.5349 time: 0.0214s\n",
      "Epoch: 0047 loss_train: 0.8406 acc_train: 0.5071 loss_val: 0.7990 acc_val: 0.5249 time: 0.0263s\n",
      "Epoch: 0048 loss_train: 0.8188 acc_train: 0.5053 loss_val: 0.8013 acc_val: 0.5100 time: 0.0185s\n",
      "Epoch: 0049 loss_train: 0.8428 acc_train: 0.5096 loss_val: 0.8061 acc_val: 0.4950 time: 0.0180s\n",
      "Epoch: 0050 loss_train: 0.8180 acc_train: 0.5024 loss_val: 0.8129 acc_val: 0.4602 time: 0.0167s\n",
      "Epoch: 0051 loss_train: 0.7731 acc_train: 0.5091 loss_val: 0.8200 acc_val: 0.4577 time: 0.0168s\n",
      "Epoch: 0052 loss_train: 0.7861 acc_train: 0.5056 loss_val: 0.8303 acc_val: 0.4577 time: 0.0170s\n",
      "Epoch: 0053 loss_train: 0.7939 acc_train: 0.4862 loss_val: 0.8367 acc_val: 0.4577 time: 0.0165s\n",
      "Epoch: 0054 loss_train: 0.7747 acc_train: 0.4872 loss_val: 0.8321 acc_val: 0.4577 time: 0.0182s\n",
      "Epoch: 0055 loss_train: 0.7991 acc_train: 0.4929 loss_val: 0.8234 acc_val: 0.4577 time: 0.0238s\n",
      "Epoch: 0056 loss_train: 0.8038 acc_train: 0.4907 loss_val: 0.8129 acc_val: 0.4602 time: 0.0218s\n",
      "Epoch: 0057 loss_train: 0.8253 acc_train: 0.4869 loss_val: 0.8144 acc_val: 0.4577 time: 0.0265s\n",
      "Epoch: 0058 loss_train: 0.8071 acc_train: 0.5091 loss_val: 0.8206 acc_val: 0.4577 time: 0.0174s\n",
      "Epoch: 0059 loss_train: 0.7596 acc_train: 0.4894 loss_val: 0.8268 acc_val: 0.4527 time: 0.0187s\n",
      "Epoch: 0060 loss_train: 0.7763 acc_train: 0.4862 loss_val: 0.8211 acc_val: 0.4527 time: 0.0204s\n",
      "Epoch: 0061 loss_train: 0.7749 acc_train: 0.4952 loss_val: 0.8137 acc_val: 0.4527 time: 0.0180s\n",
      "Epoch: 0062 loss_train: 0.7874 acc_train: 0.4939 loss_val: 0.8046 acc_val: 0.4677 time: 0.0170s\n",
      "Epoch: 0063 loss_train: 0.7762 acc_train: 0.5043 loss_val: 0.7999 acc_val: 0.4876 time: 0.0203s\n",
      "Epoch: 0064 loss_train: 0.7692 acc_train: 0.4964 loss_val: 0.7951 acc_val: 0.4950 time: 0.0215s\n",
      "Epoch: 0065 loss_train: 0.7865 acc_train: 0.5106 loss_val: 0.7900 acc_val: 0.5050 time: 0.0207s\n",
      "Epoch: 0066 loss_train: 0.7692 acc_train: 0.4941 loss_val: 0.7856 acc_val: 0.5050 time: 0.0189s\n",
      "Epoch: 0067 loss_train: 0.7643 acc_train: 0.4879 loss_val: 0.7813 acc_val: 0.5050 time: 0.0199s\n",
      "Epoch: 0068 loss_train: 0.7619 acc_train: 0.5098 loss_val: 0.7793 acc_val: 0.4925 time: 0.0213s\n",
      "Epoch: 0069 loss_train: 0.7459 acc_train: 0.5237 loss_val: 0.7819 acc_val: 0.4677 time: 0.0202s\n",
      "Epoch: 0070 loss_train: 0.7613 acc_train: 0.5068 loss_val: 0.7910 acc_val: 0.4527 time: 0.0199s\n",
      "Epoch: 0071 loss_train: 0.7538 acc_train: 0.4884 loss_val: 0.7929 acc_val: 0.4527 time: 0.0185s\n",
      "Epoch: 0072 loss_train: 0.7629 acc_train: 0.4937 loss_val: 0.7838 acc_val: 0.4527 time: 0.0171s\n",
      "Epoch: 0073 loss_train: 0.7827 acc_train: 0.4872 loss_val: 0.7668 acc_val: 0.4876 time: 0.0189s\n",
      "Epoch: 0074 loss_train: 0.7485 acc_train: 0.4976 loss_val: 0.7585 acc_val: 0.5149 time: 0.0194s\n",
      "Epoch: 0075 loss_train: 0.7853 acc_train: 0.5103 loss_val: 0.7562 acc_val: 0.5274 time: 0.0209s\n",
      "Epoch: 0076 loss_train: 0.7646 acc_train: 0.5061 loss_val: 0.7567 acc_val: 0.5100 time: 0.0184s\n",
      "Epoch: 0077 loss_train: 0.7634 acc_train: 0.5118 loss_val: 0.7618 acc_val: 0.4876 time: 0.0222s\n",
      "Epoch: 0078 loss_train: 0.7620 acc_train: 0.4817 loss_val: 0.7590 acc_val: 0.5000 time: 0.0221s\n",
      "Epoch: 0079 loss_train: 0.7463 acc_train: 0.4979 loss_val: 0.7571 acc_val: 0.5100 time: 0.0178s\n",
      "Epoch: 0080 loss_train: 0.7326 acc_train: 0.4872 loss_val: 0.7565 acc_val: 0.5100 time: 0.0172s\n",
      "Epoch: 0081 loss_train: 0.7308 acc_train: 0.5118 loss_val: 0.7582 acc_val: 0.5100 time: 0.0181s\n",
      "Epoch: 0082 loss_train: 0.7584 acc_train: 0.5220 loss_val: 0.7606 acc_val: 0.5100 time: 0.0211s\n",
      "Epoch: 0083 loss_train: 0.7548 acc_train: 0.5038 loss_val: 0.7661 acc_val: 0.4975 time: 0.0174s\n",
      "Epoch: 0084 loss_train: 0.7591 acc_train: 0.5014 loss_val: 0.7772 acc_val: 0.4776 time: 0.0211s\n",
      "Epoch: 0085 loss_train: 0.7386 acc_train: 0.5029 loss_val: 0.7736 acc_val: 0.4876 time: 0.0258s\n",
      "Epoch: 0086 loss_train: 0.7446 acc_train: 0.5133 loss_val: 0.7594 acc_val: 0.5075 time: 0.0195s\n",
      "Epoch: 0087 loss_train: 0.7348 acc_train: 0.5128 loss_val: 0.7540 acc_val: 0.5149 time: 0.0224s\n",
      "Epoch: 0088 loss_train: 0.7361 acc_train: 0.5128 loss_val: 0.7502 acc_val: 0.5174 time: 0.0212s\n",
      "Epoch: 0089 loss_train: 0.7448 acc_train: 0.5006 loss_val: 0.7469 acc_val: 0.5224 time: 0.0248s\n",
      "Epoch: 0090 loss_train: 0.7355 acc_train: 0.5165 loss_val: 0.7466 acc_val: 0.5149 time: 0.0200s\n",
      "Epoch: 0091 loss_train: 0.7393 acc_train: 0.5091 loss_val: 0.7476 acc_val: 0.5100 time: 0.0209s\n",
      "Epoch: 0092 loss_train: 0.7310 acc_train: 0.4979 loss_val: 0.7487 acc_val: 0.5124 time: 0.0182s\n",
      "Epoch: 0093 loss_train: 0.7297 acc_train: 0.5145 loss_val: 0.7468 acc_val: 0.5100 time: 0.0182s\n",
      "Epoch: 0094 loss_train: 0.7389 acc_train: 0.5024 loss_val: 0.7467 acc_val: 0.5100 time: 0.0243s\n",
      "Epoch: 0095 loss_train: 0.7288 acc_train: 0.5120 loss_val: 0.7456 acc_val: 0.5124 time: 0.0171s\n",
      "Epoch: 0096 loss_train: 0.7222 acc_train: 0.5066 loss_val: 0.7458 acc_val: 0.5025 time: 0.0180s\n",
      "Epoch: 0097 loss_train: 0.7361 acc_train: 0.5153 loss_val: 0.7442 acc_val: 0.5025 time: 0.0191s\n",
      "Epoch: 0098 loss_train: 0.7222 acc_train: 0.5153 loss_val: 0.7402 acc_val: 0.5149 time: 0.0200s\n",
      "Epoch: 0099 loss_train: 0.7407 acc_train: 0.5128 loss_val: 0.7377 acc_val: 0.5224 time: 0.0227s\n",
      "Epoch: 0100 loss_train: 0.7308 acc_train: 0.5178 loss_val: 0.7373 acc_val: 0.5249 time: 0.0212s\n",
      "Epoch: 0101 loss_train: 0.7439 acc_train: 0.5071 loss_val: 0.7367 acc_val: 0.5224 time: 0.0184s\n",
      "Epoch: 0102 loss_train: 0.7187 acc_train: 0.5118 loss_val: 0.7383 acc_val: 0.5075 time: 0.0175s\n",
      "Epoch: 0103 loss_train: 0.7245 acc_train: 0.5140 loss_val: 0.7440 acc_val: 0.5025 time: 0.0183s\n",
      "Epoch: 0104 loss_train: 0.7357 acc_train: 0.5140 loss_val: 0.7489 acc_val: 0.4975 time: 0.0188s\n",
      "Epoch: 0105 loss_train: 0.7412 acc_train: 0.5078 loss_val: 0.7402 acc_val: 0.5025 time: 0.0163s\n",
      "Epoch: 0106 loss_train: 0.7185 acc_train: 0.4991 loss_val: 0.7319 acc_val: 0.5075 time: 0.0178s\n",
      "Epoch: 0107 loss_train: 0.7174 acc_train: 0.5053 loss_val: 0.7275 acc_val: 0.5174 time: 0.0189s\n",
      "Epoch: 0108 loss_train: 0.7293 acc_train: 0.5153 loss_val: 0.7275 acc_val: 0.5224 time: 0.0194s\n",
      "Epoch: 0109 loss_train: 0.7310 acc_train: 0.5086 loss_val: 0.7299 acc_val: 0.5174 time: 0.0250s\n",
      "Epoch: 0110 loss_train: 0.7284 acc_train: 0.5205 loss_val: 0.7343 acc_val: 0.5025 time: 0.0174s\n",
      "Epoch: 0111 loss_train: 0.7443 acc_train: 0.5140 loss_val: 0.7287 acc_val: 0.5075 time: 0.0170s\n",
      "Epoch: 0112 loss_train: 0.7134 acc_train: 0.5207 loss_val: 0.7266 acc_val: 0.5174 time: 0.0172s\n",
      "Epoch: 0113 loss_train: 0.7271 acc_train: 0.5130 loss_val: 0.7280 acc_val: 0.5075 time: 0.0194s\n",
      "Epoch: 0114 loss_train: 0.7229 acc_train: 0.5021 loss_val: 0.7283 acc_val: 0.5174 time: 0.0262s\n",
      "Epoch: 0115 loss_train: 0.7268 acc_train: 0.5076 loss_val: 0.7337 acc_val: 0.5025 time: 0.0173s\n",
      "Epoch: 0116 loss_train: 0.7258 acc_train: 0.5011 loss_val: 0.7370 acc_val: 0.5025 time: 0.0175s\n",
      "Epoch: 0117 loss_train: 0.7162 acc_train: 0.5004 loss_val: 0.7360 acc_val: 0.5025 time: 0.0179s\n",
      "Epoch: 0118 loss_train: 0.7210 acc_train: 0.4984 loss_val: 0.7372 acc_val: 0.5025 time: 0.0174s\n",
      "Epoch: 0119 loss_train: 0.7242 acc_train: 0.5088 loss_val: 0.7382 acc_val: 0.5025 time: 0.0192s\n",
      "Epoch: 0120 loss_train: 0.7331 acc_train: 0.5150 loss_val: 0.7385 acc_val: 0.5025 time: 0.0262s\n",
      "Epoch: 0121 loss_train: 0.7209 acc_train: 0.4976 loss_val: 0.7397 acc_val: 0.5025 time: 0.0213s\n",
      "Epoch: 0122 loss_train: 0.7181 acc_train: 0.5108 loss_val: 0.7362 acc_val: 0.5025 time: 0.0178s\n",
      "Epoch: 0123 loss_train: 0.7534 acc_train: 0.5120 loss_val: 0.7261 acc_val: 0.5274 time: 0.0197s\n",
      "Epoch: 0124 loss_train: 0.7165 acc_train: 0.5202 loss_val: 0.7254 acc_val: 0.5225 time: 0.0176s\n",
      "Epoch: 0125 loss_train: 0.7136 acc_train: 0.5165 loss_val: 0.7248 acc_val: 0.5249 time: 0.0217s\n",
      "Epoch: 0126 loss_train: 0.7313 acc_train: 0.5168 loss_val: 0.7289 acc_val: 0.5174 time: 0.0171s\n",
      "Epoch: 0127 loss_train: 0.7166 acc_train: 0.5175 loss_val: 0.7439 acc_val: 0.4975 time: 0.0175s\n",
      "Epoch: 0128 loss_train: 0.7234 acc_train: 0.5041 loss_val: 0.7597 acc_val: 0.4826 time: 0.0178s\n",
      "Epoch: 0129 loss_train: 0.7497 acc_train: 0.4949 loss_val: 0.7475 acc_val: 0.4975 time: 0.0172s\n",
      "Epoch: 0130 loss_train: 0.7268 acc_train: 0.5056 loss_val: 0.7332 acc_val: 0.5124 time: 0.0183s\n",
      "Epoch: 0131 loss_train: 0.7294 acc_train: 0.5088 loss_val: 0.7262 acc_val: 0.5149 time: 0.0301s\n",
      "Epoch: 0132 loss_train: 0.7432 acc_train: 0.5051 loss_val: 0.7210 acc_val: 0.5224 time: 0.0177s\n",
      "Epoch: 0133 loss_train: 0.7241 acc_train: 0.5073 loss_val: 0.7191 acc_val: 0.5224 time: 0.0169s\n",
      "Epoch: 0134 loss_train: 0.7190 acc_train: 0.5163 loss_val: 0.7224 acc_val: 0.5149 time: 0.0170s\n",
      "Epoch: 0135 loss_train: 0.7266 acc_train: 0.5150 loss_val: 0.7320 acc_val: 0.5025 time: 0.0201s\n",
      "Epoch: 0136 loss_train: 0.7226 acc_train: 0.5113 loss_val: 0.7364 acc_val: 0.4975 time: 0.0196s\n",
      "Epoch: 0137 loss_train: 0.7275 acc_train: 0.5076 loss_val: 0.7367 acc_val: 0.4975 time: 0.0174s\n",
      "Epoch: 0138 loss_train: 0.7069 acc_train: 0.5053 loss_val: 0.7309 acc_val: 0.5025 time: 0.0174s\n",
      "Epoch: 0139 loss_train: 0.7211 acc_train: 0.5168 loss_val: 0.7259 acc_val: 0.5025 time: 0.0189s\n",
      "Epoch: 0140 loss_train: 0.7260 acc_train: 0.5170 loss_val: 0.7285 acc_val: 0.5025 time: 0.0219s\n",
      "Epoch: 0141 loss_train: 0.6982 acc_train: 0.5222 loss_val: 0.7303 acc_val: 0.5025 time: 0.0186s\n",
      "Epoch: 0142 loss_train: 0.7163 acc_train: 0.5115 loss_val: 0.7327 acc_val: 0.5025 time: 0.0271s\n",
      "Epoch: 0143 loss_train: 0.7295 acc_train: 0.5118 loss_val: 0.7383 acc_val: 0.4975 time: 0.0184s\n",
      "Epoch: 0144 loss_train: 0.7342 acc_train: 0.4860 loss_val: 0.7408 acc_val: 0.4975 time: 0.0190s\n",
      "Epoch: 0145 loss_train: 0.7244 acc_train: 0.5028 loss_val: 0.7354 acc_val: 0.5025 time: 0.0197s\n",
      "Epoch: 0146 loss_train: 0.7111 acc_train: 0.5165 loss_val: 0.7327 acc_val: 0.5025 time: 0.0193s\n",
      "Epoch: 0147 loss_train: 0.7336 acc_train: 0.5009 loss_val: 0.7219 acc_val: 0.5174 time: 0.0181s\n",
      "Epoch: 0148 loss_train: 0.7186 acc_train: 0.5108 loss_val: 0.7200 acc_val: 0.5225 time: 0.0179s\n",
      "Epoch: 0149 loss_train: 0.7268 acc_train: 0.5175 loss_val: 0.7201 acc_val: 0.5225 time: 0.0187s\n",
      "Epoch: 0150 loss_train: 0.7179 acc_train: 0.5175 loss_val: 0.7201 acc_val: 0.5225 time: 0.0202s\n",
      "Epoch: 0151 loss_train: 0.7321 acc_train: 0.4974 loss_val: 0.7245 acc_val: 0.5125 time: 0.0172s\n",
      "Epoch: 0152 loss_train: 0.7175 acc_train: 0.5155 loss_val: 0.7303 acc_val: 0.5025 time: 0.0179s\n",
      "Epoch: 0153 loss_train: 0.7158 acc_train: 0.5168 loss_val: 0.7334 acc_val: 0.5025 time: 0.0268s\n",
      "Epoch: 0154 loss_train: 0.7303 acc_train: 0.5098 loss_val: 0.7276 acc_val: 0.5025 time: 0.0190s\n",
      "Epoch: 0155 loss_train: 0.7229 acc_train: 0.4847 loss_val: 0.7212 acc_val: 0.5125 time: 0.0201s\n",
      "Epoch: 0156 loss_train: 0.7174 acc_train: 0.5198 loss_val: 0.7211 acc_val: 0.5174 time: 0.0165s\n",
      "Epoch: 0157 loss_train: 0.6938 acc_train: 0.5245 loss_val: 0.7221 acc_val: 0.5100 time: 0.0168s\n",
      "Epoch: 0158 loss_train: 0.7095 acc_train: 0.5113 loss_val: 0.7259 acc_val: 0.5075 time: 0.0167s\n",
      "Epoch: 0159 loss_train: 0.7125 acc_train: 0.5120 loss_val: 0.7297 acc_val: 0.5025 time: 0.0167s\n",
      "Epoch: 0160 loss_train: 0.7127 acc_train: 0.5101 loss_val: 0.7305 acc_val: 0.4975 time: 0.0173s\n",
      "Epoch: 0161 loss_train: 0.7158 acc_train: 0.5163 loss_val: 0.7332 acc_val: 0.4975 time: 0.0219s\n",
      "Epoch: 0162 loss_train: 0.7091 acc_train: 0.5098 loss_val: 0.7316 acc_val: 0.4975 time: 0.0166s\n",
      "Epoch: 0163 loss_train: 0.7188 acc_train: 0.5113 loss_val: 0.7255 acc_val: 0.5025 time: 0.0168s\n",
      "Epoch: 0164 loss_train: 0.7088 acc_train: 0.5046 loss_val: 0.7168 acc_val: 0.5124 time: 0.0235s\n",
      "Epoch: 0165 loss_train: 0.7204 acc_train: 0.5140 loss_val: 0.7133 acc_val: 0.5174 time: 0.0185s\n",
      "Epoch: 0166 loss_train: 0.7149 acc_train: 0.5138 loss_val: 0.7133 acc_val: 0.5274 time: 0.0191s\n",
      "Epoch: 0167 loss_train: 0.7187 acc_train: 0.5150 loss_val: 0.7146 acc_val: 0.5174 time: 0.0190s\n",
      "Epoch: 0168 loss_train: 0.7139 acc_train: 0.5170 loss_val: 0.7174 acc_val: 0.5075 time: 0.0180s\n",
      "Epoch: 0169 loss_train: 0.7024 acc_train: 0.5096 loss_val: 0.7224 acc_val: 0.5025 time: 0.0175s\n",
      "Epoch: 0170 loss_train: 0.7097 acc_train: 0.5118 loss_val: 0.7279 acc_val: 0.4975 time: 0.0185s\n",
      "Epoch: 0171 loss_train: 0.6976 acc_train: 0.5190 loss_val: 0.7336 acc_val: 0.4975 time: 0.0203s\n",
      "Epoch: 0172 loss_train: 0.7247 acc_train: 0.5053 loss_val: 0.7267 acc_val: 0.4975 time: 0.0182s\n",
      "Epoch: 0173 loss_train: 0.7098 acc_train: 0.5168 loss_val: 0.7239 acc_val: 0.5025 time: 0.0173s\n",
      "Epoch: 0174 loss_train: 0.7265 acc_train: 0.5056 loss_val: 0.7188 acc_val: 0.5125 time: 0.0181s\n",
      "Epoch: 0175 loss_train: 0.7126 acc_train: 0.5183 loss_val: 0.7187 acc_val: 0.5174 time: 0.0242s\n",
      "Epoch: 0176 loss_train: 0.7187 acc_train: 0.5153 loss_val: 0.7202 acc_val: 0.5125 time: 0.0219s\n",
      "Epoch: 0177 loss_train: 0.7096 acc_train: 0.5101 loss_val: 0.7246 acc_val: 0.5025 time: 0.0182s\n",
      "Epoch: 0178 loss_train: 0.7205 acc_train: 0.5163 loss_val: 0.7320 acc_val: 0.4975 time: 0.0173s\n",
      "Epoch: 0179 loss_train: 0.7161 acc_train: 0.5111 loss_val: 0.7282 acc_val: 0.5025 time: 0.0177s\n",
      "Epoch: 0180 loss_train: 0.7059 acc_train: 0.5081 loss_val: 0.7236 acc_val: 0.5050 time: 0.0206s\n",
      "Epoch: 0181 loss_train: 0.7219 acc_train: 0.5041 loss_val: 0.7221 acc_val: 0.5149 time: 0.0174s\n",
      "Epoch: 0182 loss_train: 0.7128 acc_train: 0.5063 loss_val: 0.7207 acc_val: 0.5125 time: 0.0207s\n",
      "Epoch: 0183 loss_train: 0.7049 acc_train: 0.5178 loss_val: 0.7191 acc_val: 0.5125 time: 0.0179s\n",
      "Epoch: 0184 loss_train: 0.6971 acc_train: 0.5183 loss_val: 0.7205 acc_val: 0.5000 time: 0.0176s\n",
      "Epoch: 0185 loss_train: 0.7109 acc_train: 0.5133 loss_val: 0.7217 acc_val: 0.5025 time: 0.0184s\n",
      "Epoch: 0186 loss_train: 0.7149 acc_train: 0.5073 loss_val: 0.7260 acc_val: 0.4975 time: 0.0254s\n",
      "Epoch: 0187 loss_train: 0.7060 acc_train: 0.5200 loss_val: 0.7311 acc_val: 0.4975 time: 0.0213s\n",
      "Epoch: 0188 loss_train: 0.7291 acc_train: 0.5088 loss_val: 0.7314 acc_val: 0.4975 time: 0.0176s\n",
      "Epoch: 0189 loss_train: 0.7067 acc_train: 0.5155 loss_val: 0.7289 acc_val: 0.4975 time: 0.0181s\n",
      "Epoch: 0190 loss_train: 0.7180 acc_train: 0.5103 loss_val: 0.7210 acc_val: 0.5125 time: 0.0173s\n",
      "Epoch: 0191 loss_train: 0.7213 acc_train: 0.5038 loss_val: 0.7207 acc_val: 0.5225 time: 0.0171s\n",
      "Epoch: 0192 loss_train: 0.7117 acc_train: 0.5227 loss_val: 0.7213 acc_val: 0.5225 time: 0.0198s\n",
      "Epoch: 0193 loss_train: 0.7223 acc_train: 0.5178 loss_val: 0.7220 acc_val: 0.5149 time: 0.0187s\n",
      "Epoch: 0194 loss_train: 0.7081 acc_train: 0.5252 loss_val: 0.7309 acc_val: 0.4975 time: 0.0179s\n",
      "Epoch: 0195 loss_train: 0.6992 acc_train: 0.5046 loss_val: 0.7357 acc_val: 0.4925 time: 0.0174s\n",
      "Epoch: 0196 loss_train: 0.7020 acc_train: 0.4991 loss_val: 0.7355 acc_val: 0.4925 time: 0.0191s\n",
      "Epoch: 0197 loss_train: 0.7324 acc_train: 0.4937 loss_val: 0.7216 acc_val: 0.5025 time: 0.0243s\n",
      "Epoch: 0198 loss_train: 0.7086 acc_train: 0.5088 loss_val: 0.7130 acc_val: 0.5224 time: 0.0294s\n",
      "Epoch: 0199 loss_train: 0.7178 acc_train: 0.5270 loss_val: 0.7113 acc_val: 0.5274 time: 0.0325s\n",
      "Epoch: 0200 loss_train: 0.7069 acc_train: 0.5110 loss_val: 0.7102 acc_val: 0.5224 time: 0.0689s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 3.9845s\n",
      "Test set results: loss= 0.7102 accuracy= 0.5224 unweighted accuracy= 0.7350\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from pygcn.utils import load_data, accuracy, balanced_accuracy\n",
    "from pygcn.models import GCN\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--fastmode', action='store_true', default=False,\n",
    "                    help='Validate during training pass.')\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=200,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=16,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "\n",
    "args = parser.parse_args(['--epochs=200','--dropout=0.9','--hidden=16','--weight_decay=5e-4'])\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=args.hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=args.dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "\n",
    "def balanced_accuracy(output, labels):\n",
    "    # output, labels - torch tensors\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    \n",
    "    # Calculate the frequency of each label\n",
    "    label_counts = torch.bincount(labels)\n",
    "    weights = 1.0 / label_counts.float()\n",
    "    \n",
    "    # Apply weights to the correct predictions\n",
    "    weighted_correct = correct * weights[labels]\n",
    "    weighted_accuracy = weighted_correct.sum() / weights[labels].sum()\n",
    "    \n",
    "    return weighted_accuracy\n",
    "\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = balanced_accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = balanced_accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = balanced_accuracy(output[idx_test], labels[idx_test])\n",
    "    acc_test_unbal = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()),\n",
    "          \"unweighted accuracy= {:.4f}\".format(acc_test_unbal.item()))\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFNN control - only consider self values\n",
    "adj = torch.eye(adj.size(0)).to_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 7.167838629011553e-07\n"
     ]
    }
   ],
   "source": [
    "# Method 2 - sparsen after row-normalization\n",
    "# this is the method that works for mutations\n",
    "adj = rowNormalize(aff)\n",
    "pctile=0.\n",
    "threshold = np.quantile(adj,pctile)\n",
    "print(pctile, threshold)\n",
    "adj[adj < threshold] = 0\n",
    "adj = sparse_tensor_from_arr(adj)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
