{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "# os.chdir('/home/jiageng/Documents/fhr/pipeline/')\n",
    "os.chdir('/home/jiageng/Documents/fhr/pygcn/pygcn')\n",
    "import snf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 405,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stdNormalize(df):\n",
    "    std = df.std().fillna(1)\n",
    "    mean = df - df.mean()\n",
    "    df_norm = mean / std\n",
    "    return np.array(df_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rowNormalize(mx):\n",
    "    \"\"\"Row-normalize matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = np.diag(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import coo_matrix\n",
    "\n",
    "def sparse_tensor_from_arr(arr):\n",
    "    \"\"\"Convert numpy array to torch sparse tensor\"\"\"\n",
    "    # Convert numpy array to scipy sparse matrix\n",
    "    sparse_sp = coo_matrix(arr)\n",
    "    \n",
    "    # Convert scipy sparse matrix to torch sparse tensor\n",
    "    sparse_tensor = torch.sparse_coo_tensor(\n",
    "        torch.LongTensor([sparse_sp.row, sparse_sp.col]),\n",
    "        torch.FloatTensor(sparse_sp.data),\n",
    "        torch.Size(sparse_sp.shape)\n",
    "    )\n",
    "    \n",
    "    return sparse_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_labels = pd.read_csv('/home/jiageng/Documents/fhr/annotations/fhr-annotations.tsv',sep='\\t').set_index('PUBLIC_ID').query('risk != -1')\n",
    "df_labels['risk'] += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset to samples with fhr labels\n",
    "data = pd.read_csv('/home/jiageng/Documents/fhr/matrices/gene_exp_matrix_k500.tsv',sep='\\t').set_index('PUBLIC_ID')\n",
    "if 'SAMPLE' in data.columns:\n",
    "    data = data.drop(columns=['SAMPLE'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use all samples with features, handle the missing labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "806\n"
     ]
    }
   ],
   "source": [
    "# use all rna seq samples\n",
    "public_ids = data.index\n",
    "print(len(public_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([806, 500])\n"
     ]
    }
   ],
   "source": [
    "feature_norm_mtd = '' \n",
    "\n",
    "if feature_norm_mtd == 'stdnorm':\n",
    "    features = torch.tensor(stdNormalize(data.loc[public_ids]),dtype=torch.float32)\n",
    "elif feature_norm_mtd == 'rownorm':\n",
    "    features = torch.tensor(rowNormalize(data.loc[public_ids].values),dtype=torch.float32)\n",
    "else:\n",
    "    # default is to use raw log tpm+1 values\n",
    "    features = torch.tensor(data.loc[public_ids].values,dtype=torch.float32)\n",
    "print(features.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "metadata": {},
   "outputs": [],
   "source": [
    "risk = df_labels.reindex(pd.Index(public_ids)).loc[public_ids]['risk'].fillna(-1).astype(int)\n",
    "labels = torch.tensor(risk.values,dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 408,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "risk\n",
       " 1    403\n",
       " 2    216\n",
       "-1    105\n",
       " 3     82\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 408,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# missing values are set to -1\n",
    "risk.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 410,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Un-labeled 105\n",
      "Labeled train 467\n",
      "Labeled val 234\n"
     ]
    }
   ],
   "source": [
    "idx_labeled = np.where(risk != -1)[0]\n",
    "idx_unlabeled = np.where(risk == -1)[0]\n",
    "idx_val = torch.tensor(idx_labeled[::3])\n",
    "idx_test = idx_val\n",
    "idx_train = torch.tensor([idx for idx in idx_labeled if idx not in idx_val])\n",
    "print('Un-labeled',len(idx_unlabeled))\n",
    "print('Labeled train',len(idx_train))\n",
    "print('Labeled val',len(idx_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "risk\n",
       "1    268\n",
       "2    149\n",
       "3     50\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 396,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 = SR, 2 = GHR, 3 = FHR\n",
    "risk.iloc[idx_train].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "risk\n",
       "1    135\n",
       "2     67\n",
       "3     32\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 397,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1 = SR, 2 = GHR, 3 = FHR\n",
    "# idx_val and idx_test are the same for now\n",
    "risk.iloc[idx_val].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare adjacency matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiageng/Documents/fhr/.venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n",
      "/home/jiageng/Documents/fhr/.venv/lib/python3.12/site-packages/sklearn/utils/deprecation.py:151: FutureWarning: 'force_all_finite' was renamed to 'ensure_all_finite' in 1.6 and will be removed in 1.8.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(806, 806)\n"
     ]
    }
   ],
   "source": [
    "import snf\n",
    "aff = snf.make_affinity(stdNormalize(data), metric='cosine', K=20, mu=0.5)\n",
    "print(aff.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase the sparsity of the matrix by 0-clipping small values\n",
    "\n",
    "The sparser the matrix, the higher accuracy of the GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.95 0.11617743286236207\n"
     ]
    }
   ],
   "source": [
    "pctile = 0.95\n",
    "threshold = np.quantile(aff,pctile)\n",
    "print(pctile, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [],
   "source": [
    "aff_clip = aff.copy()\n",
    "aff_clip[aff_clip < threshold] = 0\n",
    "adj = sparse_tensor_from_arr(rowNormalize(aff_clip))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0001 loss_train: 15.1131 acc_train: 0.1499 loss_val: 1.5547 acc_val: 0.5769 time: 0.0269s\n",
      "Epoch: 0002 loss_train: 2.7322 acc_train: 0.1349 loss_val: 1.3211 acc_val: 0.1624 time: 0.0241s\n",
      "Epoch: 0003 loss_train: 1.3958 acc_train: 0.0364 loss_val: 1.6631 acc_val: 0.2650 time: 0.0190s\n",
      "Epoch: 0004 loss_train: 2.3916 acc_train: 0.0621 loss_val: 1.3817 acc_val: 0.0043 time: 0.0279s\n",
      "Epoch: 0005 loss_train: 1.3961 acc_train: 0.0000 loss_val: 1.5988 acc_val: 0.2778 time: 0.0164s\n",
      "Epoch: 0006 loss_train: 2.0146 acc_train: 0.0814 loss_val: 1.3698 acc_val: 0.5769 time: 0.0095s\n",
      "Epoch: 0007 loss_train: 1.3694 acc_train: 0.5739 loss_val: 1.1264 acc_val: 0.5769 time: 0.0076s\n",
      "Epoch: 0008 loss_train: 1.3248 acc_train: 0.5675 loss_val: 1.3584 acc_val: 0.5769 time: 0.0095s\n",
      "Epoch: 0009 loss_train: 1.3553 acc_train: 0.5739 loss_val: 1.3533 acc_val: 0.5769 time: 0.0069s\n",
      "Epoch: 0010 loss_train: 1.3490 acc_train: 0.5739 loss_val: 1.3470 acc_val: 0.5769 time: 0.0093s\n",
      "Epoch: 0011 loss_train: 1.3423 acc_train: 0.5739 loss_val: 1.3408 acc_val: 0.5769 time: 0.0071s\n",
      "Epoch: 0012 loss_train: 1.3356 acc_train: 0.5739 loss_val: 1.3347 acc_val: 0.5769 time: 0.0082s\n",
      "Epoch: 0013 loss_train: 1.3289 acc_train: 0.5739 loss_val: 1.3287 acc_val: 0.5769 time: 0.0070s\n",
      "Epoch: 0014 loss_train: 1.3224 acc_train: 0.5739 loss_val: 1.3227 acc_val: 0.5769 time: 0.0081s\n",
      "Epoch: 0015 loss_train: 1.3159 acc_train: 0.5739 loss_val: 1.3168 acc_val: 0.5769 time: 0.0047s\n",
      "Epoch: 0016 loss_train: 1.3095 acc_train: 0.5739 loss_val: 1.3110 acc_val: 0.5769 time: 0.0048s\n",
      "Epoch: 0017 loss_train: 1.3032 acc_train: 0.5739 loss_val: 1.3053 acc_val: 0.5769 time: 0.0073s\n",
      "Epoch: 0018 loss_train: 1.2970 acc_train: 0.5739 loss_val: 1.2997 acc_val: 0.5769 time: 0.0046s\n",
      "Epoch: 0019 loss_train: 1.2909 acc_train: 0.5739 loss_val: 1.2942 acc_val: 0.5769 time: 0.0055s\n",
      "Epoch: 0020 loss_train: 1.2849 acc_train: 0.5739 loss_val: 1.2888 acc_val: 0.5769 time: 0.0076s\n",
      "Epoch: 0021 loss_train: 1.2789 acc_train: 0.5739 loss_val: 1.2835 acc_val: 0.5769 time: 0.0124s\n",
      "Epoch: 0022 loss_train: 1.2731 acc_train: 0.5739 loss_val: 1.2783 acc_val: 0.5769 time: 0.0083s\n",
      "Epoch: 0023 loss_train: 1.2674 acc_train: 0.5739 loss_val: 1.2731 acc_val: 0.5769 time: 0.0079s\n",
      "Epoch: 0024 loss_train: 1.2618 acc_train: 0.5739 loss_val: 1.2683 acc_val: 0.5769 time: 0.0082s\n",
      "Epoch: 0025 loss_train: 1.2568 acc_train: 0.5739 loss_val: 1.2632 acc_val: 0.5769 time: 0.0099s\n",
      "Epoch: 0026 loss_train: 1.2509 acc_train: 0.5739 loss_val: 1.2584 acc_val: 0.5769 time: 0.0065s\n",
      "Epoch: 0027 loss_train: 1.2456 acc_train: 0.5739 loss_val: 1.2536 acc_val: 0.5769 time: 0.0071s\n",
      "Epoch: 0028 loss_train: 1.2403 acc_train: 0.5739 loss_val: 1.2494 acc_val: 0.5769 time: 0.0081s\n",
      "Epoch: 0029 loss_train: 1.2355 acc_train: 0.5739 loss_val: 1.2445 acc_val: 0.5769 time: 0.0055s\n",
      "Epoch: 0030 loss_train: 1.2302 acc_train: 0.5739 loss_val: 1.2400 acc_val: 0.5769 time: 0.0056s\n",
      "Epoch: 0031 loss_train: 1.2253 acc_train: 0.5739 loss_val: 1.2357 acc_val: 0.5769 time: 0.0078s\n",
      "Epoch: 0032 loss_train: 1.2205 acc_train: 0.5739 loss_val: 1.2314 acc_val: 0.5769 time: 0.0131s\n",
      "Epoch: 0033 loss_train: 1.2158 acc_train: 0.5739 loss_val: 1.2273 acc_val: 0.5769 time: 0.0081s\n",
      "Epoch: 0034 loss_train: 1.2112 acc_train: 0.5739 loss_val: 1.2232 acc_val: 0.5769 time: 0.0072s\n",
      "Epoch: 0035 loss_train: 1.2067 acc_train: 0.5739 loss_val: 1.2192 acc_val: 0.5769 time: 0.0073s\n",
      "Epoch: 0036 loss_train: 1.2023 acc_train: 0.5739 loss_val: 1.2153 acc_val: 0.5769 time: 0.0061s\n",
      "Epoch: 0037 loss_train: 1.1979 acc_train: 0.5739 loss_val: 1.2115 acc_val: 0.5769 time: 0.0099s\n",
      "Epoch: 0038 loss_train: 1.1937 acc_train: 0.5739 loss_val: 1.2077 acc_val: 0.5769 time: 0.0081s\n",
      "Epoch: 0039 loss_train: 1.1896 acc_train: 0.5739 loss_val: 1.2041 acc_val: 0.5769 time: 0.0087s\n",
      "Epoch: 0040 loss_train: 1.1855 acc_train: 0.5739 loss_val: 1.2005 acc_val: 0.5769 time: 0.0097s\n",
      "Epoch: 0041 loss_train: 1.1815 acc_train: 0.5739 loss_val: 1.1970 acc_val: 0.5769 time: 0.0123s\n",
      "Epoch: 0042 loss_train: 1.1776 acc_train: 0.5739 loss_val: 1.1936 acc_val: 0.5769 time: 0.0088s\n",
      "Epoch: 0043 loss_train: 1.1738 acc_train: 0.5739 loss_val: 1.1902 acc_val: 0.5769 time: 0.0119s\n",
      "Epoch: 0044 loss_train: 1.1701 acc_train: 0.5739 loss_val: 1.1870 acc_val: 0.5769 time: 0.0219s\n",
      "Epoch: 0045 loss_train: 1.1665 acc_train: 0.5739 loss_val: 1.1838 acc_val: 0.5769 time: 0.0158s\n",
      "Epoch: 0046 loss_train: 1.1629 acc_train: 0.5739 loss_val: 1.1806 acc_val: 0.5769 time: 0.0229s\n",
      "Epoch: 0047 loss_train: 1.1594 acc_train: 0.5739 loss_val: 1.1776 acc_val: 0.5769 time: 0.0154s\n",
      "Epoch: 0048 loss_train: 1.1560 acc_train: 0.5739 loss_val: 1.1746 acc_val: 0.5769 time: 0.0225s\n",
      "Epoch: 0049 loss_train: 1.1527 acc_train: 0.5739 loss_val: 1.1716 acc_val: 0.5769 time: 0.0209s\n",
      "Epoch: 0050 loss_train: 1.1494 acc_train: 0.5739 loss_val: 1.1688 acc_val: 0.5769 time: 0.0374s\n",
      "Epoch: 0051 loss_train: 1.1462 acc_train: 0.5739 loss_val: 1.1659 acc_val: 0.5769 time: 0.0165s\n",
      "Epoch: 0052 loss_train: 1.1431 acc_train: 0.5739 loss_val: 1.1632 acc_val: 0.5769 time: 0.0066s\n",
      "Epoch: 0053 loss_train: 1.1400 acc_train: 0.5739 loss_val: 1.1605 acc_val: 0.5769 time: 0.0133s\n",
      "Epoch: 0054 loss_train: 1.1370 acc_train: 0.5739 loss_val: 1.1578 acc_val: 0.5769 time: 0.0108s\n",
      "Epoch: 0055 loss_train: 1.1341 acc_train: 0.5739 loss_val: 1.1553 acc_val: 0.5769 time: 0.0107s\n",
      "Epoch: 0056 loss_train: 1.1312 acc_train: 0.5739 loss_val: 1.1527 acc_val: 0.5769 time: 0.0232s\n",
      "Epoch: 0057 loss_train: 1.1284 acc_train: 0.5739 loss_val: 1.1502 acc_val: 0.5769 time: 0.0186s\n",
      "Epoch: 0058 loss_train: 1.1257 acc_train: 0.5739 loss_val: 1.1478 acc_val: 0.5769 time: 0.0115s\n",
      "Epoch: 0059 loss_train: 1.1230 acc_train: 0.5739 loss_val: 1.1454 acc_val: 0.5769 time: 0.0148s\n",
      "Epoch: 0060 loss_train: 1.1203 acc_train: 0.5739 loss_val: 1.1431 acc_val: 0.5769 time: 0.0125s\n",
      "Epoch: 0061 loss_train: 1.1178 acc_train: 0.5739 loss_val: 1.1408 acc_val: 0.5769 time: 0.0271s\n",
      "Epoch: 0062 loss_train: 1.1152 acc_train: 0.5739 loss_val: 1.1385 acc_val: 0.5769 time: 0.0182s\n",
      "Epoch: 0063 loss_train: 1.1127 acc_train: 0.5739 loss_val: 1.1363 acc_val: 0.5769 time: 0.0184s\n",
      "Epoch: 0064 loss_train: 1.1103 acc_train: 0.5739 loss_val: 1.1341 acc_val: 0.5769 time: 0.0131s\n",
      "Epoch: 0065 loss_train: 1.1079 acc_train: 0.5739 loss_val: 1.1320 acc_val: 0.5769 time: 0.0058s\n",
      "Epoch: 0066 loss_train: 1.1056 acc_train: 0.5739 loss_val: 1.1299 acc_val: 0.5769 time: 0.0108s\n",
      "Epoch: 0067 loss_train: 1.1033 acc_train: 0.5739 loss_val: 1.1279 acc_val: 0.5769 time: 0.0083s\n",
      "Epoch: 0068 loss_train: 1.1011 acc_train: 0.5739 loss_val: 1.1259 acc_val: 0.5769 time: 0.0132s\n",
      "Epoch: 0069 loss_train: 1.0989 acc_train: 0.5739 loss_val: 1.1239 acc_val: 0.5769 time: 0.0122s\n",
      "Epoch: 0070 loss_train: 1.0967 acc_train: 0.5739 loss_val: 1.1219 acc_val: 0.5769 time: 0.0199s\n",
      "Epoch: 0071 loss_train: 1.0946 acc_train: 0.5739 loss_val: 1.1200 acc_val: 0.5769 time: 0.0144s\n",
      "Epoch: 0072 loss_train: 1.0925 acc_train: 0.5739 loss_val: 1.1182 acc_val: 0.5769 time: 0.0130s\n",
      "Epoch: 0073 loss_train: 1.0905 acc_train: 0.5739 loss_val: 1.1163 acc_val: 0.5769 time: 0.0136s\n",
      "Epoch: 0074 loss_train: 1.0885 acc_train: 0.5739 loss_val: 1.1145 acc_val: 0.5769 time: 0.0103s\n",
      "Epoch: 0075 loss_train: 1.0865 acc_train: 0.5739 loss_val: 1.1127 acc_val: 0.5769 time: 0.0119s\n",
      "Epoch: 0076 loss_train: 1.0846 acc_train: 0.5739 loss_val: 1.1110 acc_val: 0.5769 time: 0.0149s\n",
      "Epoch: 0077 loss_train: 1.0827 acc_train: 0.5739 loss_val: 1.1093 acc_val: 0.5769 time: 0.0123s\n",
      "Epoch: 0078 loss_train: 1.0809 acc_train: 0.5739 loss_val: 1.1076 acc_val: 0.5769 time: 0.0051s\n",
      "Epoch: 0079 loss_train: 1.0790 acc_train: 0.5739 loss_val: 1.1059 acc_val: 0.5769 time: 0.0133s\n",
      "Epoch: 0080 loss_train: 1.0773 acc_train: 0.5739 loss_val: 1.1043 acc_val: 0.5769 time: 0.0069s\n",
      "Epoch: 0081 loss_train: 1.0755 acc_train: 0.5739 loss_val: 1.1027 acc_val: 0.5769 time: 0.0111s\n",
      "Epoch: 0082 loss_train: 1.0738 acc_train: 0.5739 loss_val: 1.1011 acc_val: 0.5769 time: 0.0111s\n",
      "Epoch: 0083 loss_train: 1.0721 acc_train: 0.5739 loss_val: 1.0995 acc_val: 0.5769 time: 0.0114s\n",
      "Epoch: 0084 loss_train: 1.0704 acc_train: 0.5739 loss_val: 1.0980 acc_val: 0.5769 time: 0.0115s\n",
      "Epoch: 0085 loss_train: 1.0688 acc_train: 0.5739 loss_val: 1.0965 acc_val: 0.5769 time: 0.0136s\n",
      "Epoch: 0086 loss_train: 1.0672 acc_train: 0.5739 loss_val: 1.0950 acc_val: 0.5769 time: 0.0143s\n",
      "Epoch: 0087 loss_train: 1.0656 acc_train: 0.5739 loss_val: 1.0935 acc_val: 0.5769 time: 0.0144s\n",
      "Epoch: 0088 loss_train: 1.0640 acc_train: 0.5739 loss_val: 1.0921 acc_val: 0.5769 time: 0.0143s\n",
      "Epoch: 0089 loss_train: 1.0625 acc_train: 0.5739 loss_val: 1.0906 acc_val: 0.5769 time: 0.0120s\n",
      "Epoch: 0090 loss_train: 1.0610 acc_train: 0.5739 loss_val: 1.0892 acc_val: 0.5769 time: 0.0112s\n",
      "Epoch: 0091 loss_train: 1.0595 acc_train: 0.5739 loss_val: 1.0878 acc_val: 0.5769 time: 0.0145s\n",
      "Epoch: 0092 loss_train: 1.0581 acc_train: 0.5739 loss_val: 1.0865 acc_val: 0.5769 time: 0.0144s\n",
      "Epoch: 0093 loss_train: 1.0567 acc_train: 0.5739 loss_val: 1.0851 acc_val: 0.5769 time: 0.0126s\n",
      "Epoch: 0094 loss_train: 1.0553 acc_train: 0.5739 loss_val: 1.0838 acc_val: 0.5769 time: 0.0088s\n",
      "Epoch: 0095 loss_train: 1.0539 acc_train: 0.5739 loss_val: 1.0825 acc_val: 0.5769 time: 0.0114s\n",
      "Epoch: 0096 loss_train: 1.0525 acc_train: 0.5739 loss_val: 1.0812 acc_val: 0.5769 time: 0.0113s\n",
      "Epoch: 0097 loss_train: 1.0512 acc_train: 0.5739 loss_val: 1.0800 acc_val: 0.5769 time: 0.0091s\n",
      "Epoch: 0098 loss_train: 1.0499 acc_train: 0.5739 loss_val: 1.0787 acc_val: 0.5769 time: 0.0115s\n",
      "Epoch: 0099 loss_train: 1.0486 acc_train: 0.5739 loss_val: 1.0775 acc_val: 0.5769 time: 0.0053s\n",
      "Epoch: 0100 loss_train: 1.0473 acc_train: 0.5739 loss_val: 1.0763 acc_val: 0.5769 time: 0.0115s\n",
      "Epoch: 0101 loss_train: 1.0460 acc_train: 0.5739 loss_val: 1.0751 acc_val: 0.5769 time: 0.0106s\n",
      "Epoch: 0102 loss_train: 1.0448 acc_train: 0.5739 loss_val: 1.0739 acc_val: 0.5769 time: 0.0085s\n",
      "Epoch: 0103 loss_train: 1.0436 acc_train: 0.5739 loss_val: 1.0728 acc_val: 0.5769 time: 0.0091s\n",
      "Epoch: 0104 loss_train: 1.0424 acc_train: 0.5739 loss_val: 1.0716 acc_val: 0.5769 time: 0.0087s\n",
      "Epoch: 0105 loss_train: 1.0412 acc_train: 0.5739 loss_val: 1.0705 acc_val: 0.5769 time: 0.0171s\n",
      "Epoch: 0106 loss_train: 1.0400 acc_train: 0.5739 loss_val: 1.0694 acc_val: 0.5769 time: 0.0136s\n",
      "Epoch: 0107 loss_train: 1.0389 acc_train: 0.5739 loss_val: 1.0683 acc_val: 0.5769 time: 0.0131s\n",
      "Epoch: 0108 loss_train: 1.0378 acc_train: 0.5739 loss_val: 1.0672 acc_val: 0.5769 time: 0.0131s\n",
      "Epoch: 0109 loss_train: 1.0367 acc_train: 0.5739 loss_val: 1.0661 acc_val: 0.5769 time: 0.0100s\n",
      "Epoch: 0110 loss_train: 1.0356 acc_train: 0.5739 loss_val: 1.0651 acc_val: 0.5769 time: 0.0143s\n",
      "Epoch: 0111 loss_train: 1.0345 acc_train: 0.5739 loss_val: 1.0640 acc_val: 0.5769 time: 0.0143s\n",
      "Epoch: 0112 loss_train: 1.0334 acc_train: 0.5739 loss_val: 1.0630 acc_val: 0.5769 time: 0.0179s\n",
      "Epoch: 0113 loss_train: 1.0324 acc_train: 0.5739 loss_val: 1.0620 acc_val: 0.5769 time: 0.0137s\n",
      "Epoch: 0114 loss_train: 1.0314 acc_train: 0.5739 loss_val: 1.0610 acc_val: 0.5769 time: 0.0147s\n",
      "Epoch: 0115 loss_train: 1.0304 acc_train: 0.5739 loss_val: 1.0600 acc_val: 0.5769 time: 0.0188s\n",
      "Epoch: 0116 loss_train: 1.0294 acc_train: 0.5739 loss_val: 1.0590 acc_val: 0.5769 time: 0.0066s\n",
      "Epoch: 0117 loss_train: 1.0284 acc_train: 0.5739 loss_val: 1.0581 acc_val: 0.5769 time: 0.0101s\n",
      "Epoch: 0118 loss_train: 1.0274 acc_train: 0.5739 loss_val: 1.0571 acc_val: 0.5769 time: 0.0139s\n",
      "Epoch: 0119 loss_train: 1.0264 acc_train: 0.5739 loss_val: 1.0562 acc_val: 0.5769 time: 0.0061s\n",
      "Epoch: 0120 loss_train: 1.0255 acc_train: 0.5739 loss_val: 1.0553 acc_val: 0.5769 time: 0.0131s\n",
      "Epoch: 0121 loss_train: 1.0246 acc_train: 0.5739 loss_val: 1.0544 acc_val: 0.5769 time: 0.0165s\n",
      "Epoch: 0122 loss_train: 1.0236 acc_train: 0.5739 loss_val: 1.0535 acc_val: 0.5769 time: 0.0142s\n",
      "Epoch: 0123 loss_train: 1.0227 acc_train: 0.5739 loss_val: 1.0526 acc_val: 0.5769 time: 0.0136s\n",
      "Epoch: 0124 loss_train: 1.0218 acc_train: 0.5739 loss_val: 1.0517 acc_val: 0.5769 time: 0.0090s\n",
      "Epoch: 0125 loss_train: 1.0210 acc_train: 0.5739 loss_val: 1.0508 acc_val: 0.5769 time: 0.0115s\n",
      "Epoch: 0126 loss_train: 1.0201 acc_train: 0.5739 loss_val: 1.0500 acc_val: 0.5769 time: 0.0140s\n",
      "Epoch: 0127 loss_train: 1.0192 acc_train: 0.5739 loss_val: 1.0492 acc_val: 0.5769 time: 0.0049s\n",
      "Epoch: 0128 loss_train: 1.0184 acc_train: 0.5739 loss_val: 1.0483 acc_val: 0.5769 time: 0.0123s\n",
      "Epoch: 0129 loss_train: 1.0176 acc_train: 0.5739 loss_val: 1.0475 acc_val: 0.5769 time: 0.0057s\n",
      "Epoch: 0130 loss_train: 1.0167 acc_train: 0.5739 loss_val: 1.0467 acc_val: 0.5769 time: 0.0111s\n",
      "Epoch: 0131 loss_train: 1.0159 acc_train: 0.5739 loss_val: 1.0459 acc_val: 0.5769 time: 0.0065s\n",
      "Epoch: 0132 loss_train: 1.0151 acc_train: 0.5739 loss_val: 1.0451 acc_val: 0.5769 time: 0.0099s\n",
      "Epoch: 0133 loss_train: 1.0144 acc_train: 0.5739 loss_val: 1.0443 acc_val: 0.5769 time: 0.0074s\n",
      "Epoch: 0134 loss_train: 1.0136 acc_train: 0.5739 loss_val: 1.0436 acc_val: 0.5769 time: 0.0103s\n",
      "Epoch: 0135 loss_train: 1.0128 acc_train: 0.5739 loss_val: 1.0428 acc_val: 0.5769 time: 0.0144s\n",
      "Epoch: 0136 loss_train: 1.0120 acc_train: 0.5739 loss_val: 1.0421 acc_val: 0.5769 time: 0.0142s\n",
      "Epoch: 0137 loss_train: 1.0113 acc_train: 0.5739 loss_val: 1.0413 acc_val: 0.5769 time: 0.0052s\n",
      "Epoch: 0138 loss_train: 1.0106 acc_train: 0.5739 loss_val: 1.0406 acc_val: 0.5769 time: 0.0120s\n",
      "Epoch: 0139 loss_train: 1.0098 acc_train: 0.5739 loss_val: 1.0399 acc_val: 0.5769 time: 0.0048s\n",
      "Epoch: 0140 loss_train: 1.0091 acc_train: 0.5739 loss_val: 1.0392 acc_val: 0.5769 time: 0.0126s\n",
      "Epoch: 0141 loss_train: 1.0084 acc_train: 0.5739 loss_val: 1.0385 acc_val: 0.5769 time: 0.0149s\n",
      "Epoch: 0142 loss_train: 1.0077 acc_train: 0.5739 loss_val: 1.0378 acc_val: 0.5769 time: 0.0108s\n",
      "Epoch: 0143 loss_train: 1.0070 acc_train: 0.5739 loss_val: 1.0371 acc_val: 0.5769 time: 0.0105s\n",
      "Epoch: 0144 loss_train: 1.0063 acc_train: 0.5739 loss_val: 1.0364 acc_val: 0.5769 time: 0.0118s\n",
      "Epoch: 0145 loss_train: 1.0057 acc_train: 0.5739 loss_val: 1.0357 acc_val: 0.5769 time: 0.0135s\n",
      "Epoch: 0146 loss_train: 1.0050 acc_train: 0.5739 loss_val: 1.0351 acc_val: 0.5769 time: 0.0135s\n",
      "Epoch: 0147 loss_train: 1.0043 acc_train: 0.5739 loss_val: 1.0344 acc_val: 0.5769 time: 0.0088s\n",
      "Epoch: 0148 loss_train: 1.0037 acc_train: 0.5739 loss_val: 1.0338 acc_val: 0.5769 time: 0.0125s\n",
      "Epoch: 0149 loss_train: 1.0030 acc_train: 0.5739 loss_val: 1.0332 acc_val: 0.5769 time: 0.0047s\n",
      "Epoch: 0150 loss_train: 1.0024 acc_train: 0.5739 loss_val: 1.0325 acc_val: 0.5769 time: 0.0121s\n",
      "Epoch: 0151 loss_train: 1.0018 acc_train: 0.5739 loss_val: 1.0319 acc_val: 0.5769 time: 0.0065s\n",
      "Epoch: 0152 loss_train: 1.0012 acc_train: 0.5739 loss_val: 1.0313 acc_val: 0.5769 time: 0.0122s\n",
      "Epoch: 0153 loss_train: 1.0006 acc_train: 0.5739 loss_val: 1.0307 acc_val: 0.5769 time: 0.0122s\n",
      "Epoch: 0154 loss_train: 1.0000 acc_train: 0.5739 loss_val: 1.0301 acc_val: 0.5769 time: 0.0083s\n",
      "Epoch: 0155 loss_train: 0.9994 acc_train: 0.5739 loss_val: 1.0295 acc_val: 0.5769 time: 0.0109s\n",
      "Epoch: 0156 loss_train: 0.9988 acc_train: 0.5739 loss_val: 1.0289 acc_val: 0.5769 time: 0.0084s\n",
      "Epoch: 0157 loss_train: 0.9982 acc_train: 0.5739 loss_val: 1.0284 acc_val: 0.5769 time: 0.0082s\n",
      "Epoch: 0158 loss_train: 0.9976 acc_train: 0.5739 loss_val: 1.0278 acc_val: 0.5769 time: 0.0092s\n",
      "Epoch: 0159 loss_train: 0.9971 acc_train: 0.5739 loss_val: 1.0272 acc_val: 0.5769 time: 0.0073s\n",
      "Epoch: 0160 loss_train: 0.9965 acc_train: 0.5739 loss_val: 1.0267 acc_val: 0.5769 time: 0.0105s\n",
      "Epoch: 0161 loss_train: 0.9960 acc_train: 0.5739 loss_val: 1.0261 acc_val: 0.5769 time: 0.0110s\n",
      "Epoch: 0162 loss_train: 0.9954 acc_train: 0.5739 loss_val: 1.0256 acc_val: 0.5769 time: 0.0148s\n",
      "Epoch: 0163 loss_train: 0.9949 acc_train: 0.5739 loss_val: 1.0250 acc_val: 0.5769 time: 0.0179s\n",
      "Epoch: 0164 loss_train: 0.9943 acc_train: 0.5739 loss_val: 1.0245 acc_val: 0.5769 time: 0.0124s\n",
      "Epoch: 0165 loss_train: 0.9938 acc_train: 0.5739 loss_val: 1.0240 acc_val: 0.5769 time: 0.0123s\n",
      "Epoch: 0166 loss_train: 0.9933 acc_train: 0.5739 loss_val: 1.0235 acc_val: 0.5769 time: 0.0116s\n",
      "Epoch: 0167 loss_train: 0.9928 acc_train: 0.5739 loss_val: 1.0230 acc_val: 0.5769 time: 0.0088s\n",
      "Epoch: 0168 loss_train: 0.9923 acc_train: 0.5739 loss_val: 1.0225 acc_val: 0.5769 time: 0.0119s\n",
      "Epoch: 0169 loss_train: 0.9918 acc_train: 0.5739 loss_val: 1.0220 acc_val: 0.5769 time: 0.0071s\n",
      "Epoch: 0170 loss_train: 0.9913 acc_train: 0.5739 loss_val: 1.0215 acc_val: 0.5769 time: 0.0109s\n",
      "Epoch: 0171 loss_train: 0.9908 acc_train: 0.5739 loss_val: 1.0210 acc_val: 0.5769 time: 0.0099s\n",
      "Epoch: 0172 loss_train: 0.9903 acc_train: 0.5739 loss_val: 1.0205 acc_val: 0.5769 time: 0.0090s\n",
      "Epoch: 0173 loss_train: 0.9898 acc_train: 0.5739 loss_val: 1.0200 acc_val: 0.5769 time: 0.0127s\n",
      "Epoch: 0174 loss_train: 0.9893 acc_train: 0.5739 loss_val: 1.0195 acc_val: 0.5769 time: 0.0099s\n",
      "Epoch: 0175 loss_train: 0.9889 acc_train: 0.5739 loss_val: 1.0191 acc_val: 0.5769 time: 0.0099s\n",
      "Epoch: 0176 loss_train: 0.9884 acc_train: 0.5739 loss_val: 1.0186 acc_val: 0.5769 time: 0.0121s\n",
      "Epoch: 0177 loss_train: 0.9879 acc_train: 0.5739 loss_val: 1.0182 acc_val: 0.5769 time: 0.0068s\n",
      "Epoch: 0178 loss_train: 0.9875 acc_train: 0.5739 loss_val: 1.0177 acc_val: 0.5769 time: 0.0113s\n",
      "Epoch: 0179 loss_train: 0.9870 acc_train: 0.5739 loss_val: 1.0173 acc_val: 0.5769 time: 0.0115s\n",
      "Epoch: 0180 loss_train: 0.9866 acc_train: 0.5739 loss_val: 1.0168 acc_val: 0.5769 time: 0.0076s\n",
      "Epoch: 0181 loss_train: 0.9861 acc_train: 0.5739 loss_val: 1.0164 acc_val: 0.5769 time: 0.0125s\n",
      "Epoch: 0182 loss_train: 0.9857 acc_train: 0.5739 loss_val: 1.0160 acc_val: 0.5769 time: 0.0103s\n",
      "Epoch: 0183 loss_train: 0.9853 acc_train: 0.5739 loss_val: 1.0155 acc_val: 0.5769 time: 0.0081s\n",
      "Epoch: 0184 loss_train: 0.9849 acc_train: 0.5739 loss_val: 1.0151 acc_val: 0.5769 time: 0.0136s\n",
      "Epoch: 0185 loss_train: 0.9844 acc_train: 0.5739 loss_val: 1.0147 acc_val: 0.5769 time: 0.0123s\n",
      "Epoch: 0186 loss_train: 0.9840 acc_train: 0.5739 loss_val: 1.0143 acc_val: 0.5769 time: 0.0083s\n",
      "Epoch: 0187 loss_train: 0.9836 acc_train: 0.5739 loss_val: 1.0139 acc_val: 0.5769 time: 0.0122s\n",
      "Epoch: 0188 loss_train: 0.9832 acc_train: 0.5739 loss_val: 1.0135 acc_val: 0.5769 time: 0.0078s\n",
      "Epoch: 0189 loss_train: 0.9828 acc_train: 0.5739 loss_val: 1.0131 acc_val: 0.5769 time: 0.0105s\n",
      "Epoch: 0190 loss_train: 0.9824 acc_train: 0.5739 loss_val: 1.0127 acc_val: 0.5769 time: 0.0101s\n",
      "Epoch: 0191 loss_train: 0.9820 acc_train: 0.5739 loss_val: 1.0123 acc_val: 0.5769 time: 0.0076s\n",
      "Epoch: 0192 loss_train: 0.9816 acc_train: 0.5739 loss_val: 1.0119 acc_val: 0.5769 time: 0.0126s\n",
      "Epoch: 0193 loss_train: 0.9812 acc_train: 0.5739 loss_val: 1.0115 acc_val: 0.5769 time: 0.0099s\n",
      "Epoch: 0194 loss_train: 0.9808 acc_train: 0.5739 loss_val: 1.0111 acc_val: 0.5769 time: 0.0096s\n",
      "Epoch: 0195 loss_train: 0.9805 acc_train: 0.5739 loss_val: 1.0108 acc_val: 0.5769 time: 0.0120s\n",
      "Epoch: 0196 loss_train: 0.9801 acc_train: 0.5739 loss_val: 1.0104 acc_val: 0.5769 time: 0.0068s\n",
      "Epoch: 0197 loss_train: 0.9797 acc_train: 0.5739 loss_val: 1.0100 acc_val: 0.5769 time: 0.0113s\n",
      "Epoch: 0198 loss_train: 0.9794 acc_train: 0.5739 loss_val: 1.0097 acc_val: 0.5769 time: 0.0094s\n",
      "Epoch: 0199 loss_train: 0.9790 acc_train: 0.5739 loss_val: 1.0093 acc_val: 0.5769 time: 0.0091s\n",
      "Epoch: 0200 loss_train: 0.9786 acc_train: 0.5739 loss_val: 1.0089 acc_val: 0.5769 time: 0.0118s\n",
      "Optimization Finished!\n",
      "Total time elapsed: 2.3633s\n",
      "Test set results: loss= 1.0089 accuracy= 0.5769\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import argparse\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from pygcn.utils import load_data, accuracy\n",
    "from pygcn.models import GCN\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--no-cuda', action='store_true', default=False,\n",
    "                    help='Disables CUDA training.')\n",
    "parser.add_argument('--fastmode', action='store_true', default=False,\n",
    "                    help='Validate during training pass.')\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=200,\n",
    "                    help='Number of epochs to train.')\n",
    "parser.add_argument('--lr', type=float, default=0.01,\n",
    "                    help='Initial learning rate.')\n",
    "parser.add_argument('--weight_decay', type=float, default=5e-4,\n",
    "                    help='Weight decay (L2 loss on parameters).')\n",
    "parser.add_argument('--hidden', type=int, default=16,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout rate (1 - keep probability).')\n",
    "\n",
    "args = parser.parse_args(['--epochs=200','--dropout=0.8','--hidden=16','--weight_decay=5e-4','--seed=1023420948521123'])\n",
    "args.cuda = not args.no_cuda and torch.cuda.is_available()\n",
    "model = GCN(nfeat=features.shape[1],\n",
    "            nhid=args.hidden,\n",
    "            nclass=labels.max().item() + 1,\n",
    "            dropout=args.dropout)\n",
    "optimizer = optim.Adam(model.parameters(),\n",
    "                       lr=args.lr, weight_decay=args.weight_decay)\n",
    "def train(epoch):\n",
    "    t = time.time()\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    output = model(features, adj)\n",
    "    loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "    acc_train = accuracy(output[idx_train], labels[idx_train])\n",
    "    loss_train.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if not args.fastmode:\n",
    "        # Evaluate validation set performance separately,\n",
    "        # deactivates dropout during validation run.\n",
    "        model.eval()\n",
    "        output = model(features, adj)\n",
    "\n",
    "    loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "    acc_val = accuracy(output[idx_val], labels[idx_val])\n",
    "    print('Epoch: {:04d}'.format(epoch+1),\n",
    "          'loss_train: {:.4f}'.format(loss_train.item()),\n",
    "          'acc_train: {:.4f}'.format(acc_train.item()),\n",
    "          'loss_val: {:.4f}'.format(loss_val.item()),\n",
    "          'acc_val: {:.4f}'.format(acc_val.item()),\n",
    "          'time: {:.4f}s'.format(time.time() - t))\n",
    "\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    output = model(features, adj)\n",
    "    loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
    "    acc_test = accuracy(output[idx_test], labels[idx_test])\n",
    "    print(\"Test set results:\",\n",
    "          \"loss= {:.4f}\".format(loss_test.item()),\n",
    "          \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "\n",
    "# Train model\n",
    "t_total = time.time()\n",
    "for epoch in range(args.epochs):\n",
    "    train(epoch)\n",
    "print(\"Optimization Finished!\")\n",
    "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
    "\n",
    "# Testing\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFNN control - only consider self values\n",
    "adj = torch.eye(adj.size(0)).to_sparse()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
